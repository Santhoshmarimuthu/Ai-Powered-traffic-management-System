{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69d29fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.106-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (1.26.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (2.28.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (1.13.0+cu117)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (0.14.0+cu117)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from ultralytics) (5.9.8)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/santhosh/.local/lib/python3.10/site-packages (from ultralytics) (0.13.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/santhosh/.local/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/santhosh/.local/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/santhosh/.local/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/santhosh/.local/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/santhosh/.local/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/santhosh/.local/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/santhosh/.local/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/santhosh/.local/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/santhosh/.local/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/santhosh/.local/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/santhosh/.local/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: typing-extensions in /home/santhosh/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/santhosh/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Downloading ultralytics-8.3.106-py3-none-any.whl (972 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.0/973.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo, ultralytics-thop, ultralytics\n",
      "Successfully installed py-cpuinfo-9.0.0 ultralytics-8.3.106 ultralytics-thop-2.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b784ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960da5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/gen-efficientnet-pytorch/zipball/master\" to /home/santhosh/.cache/torch/hub/master.zip\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite3-b733e338.pth\" to /home/santhosh/.cache/torch/hub/checkpoints/tf_efficientnet_lite3-b733e338.pth\n",
      "Downloading: \"https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_small_256.pt\" to /home/santhosh/.cache/torch/hub/checkpoints/midas_v21_small_256.pt\n",
      "100%|██████████| 81.8M/81.8M [01:23<00:00, 1.03MB/s]\n",
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "yolo_model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas.eval()\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").small_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313eb25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point 1: 522, 0\n",
      "Point 2: 842, 1\n",
      "Point 3: 1274, 712\n",
      "Point 4: 2, 712\n",
      "ROI drawing completed. Right-click to save and exit.\n",
      "Right click detected. Exiting and saving ROI points.\n",
      "ROI points saved to 'roi_points.npy' at /mnt/c/Users/santhosh/OneDrive/Desktop/trafficmanagement/roi_points.npy\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "roi_points = []\n",
    "drawing_complete = [False]\n",
    "exit_requested = [False]\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and not drawing_complete[0]:\n",
    "        if len(roi_points) < 4:\n",
    "            roi_points.append((x, y))\n",
    "            print(f\"Point {len(roi_points)}: {x}, {y}\")\n",
    "            if len(roi_points) == 4:\n",
    "                drawing_complete[0] = True\n",
    "                print(\"ROI drawing completed. Right-click to save and exit.\")\n",
    "\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN and drawing_complete[0]:\n",
    "        exit_requested[0] = True\n",
    "        print(\"Right click detected. Exiting and saving ROI points.\")\n",
    "\n",
    "# Load first frame from video\n",
    "cap = cv2.VideoCapture(\"traffic testvideo.mp4\")\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Failed to load video.\")\n",
    "    exit()\n",
    "\n",
    "cv2.namedWindow(\"Select ROI - Click 4 Points\")\n",
    "cv2.setMouseCallback(\"Select ROI - Click 4 Points\", mouse_callback)\n",
    "\n",
    "while True:\n",
    "    display_frame = frame.copy()\n",
    "\n",
    "    # Draw points\n",
    "    for point in roi_points:\n",
    "        cv2.circle(display_frame, point, 5, (0, 255, 255), -1)\n",
    "\n",
    "    # Draw rectangle when 4 points selected\n",
    "    if len(roi_points) == 4:\n",
    "        cv2.polylines(display_frame, [np.array(roi_points + [roi_points[0]])], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    cv2.imshow(\"Select ROI - Click 4 Points\", display_frame)\n",
    "\n",
    "    if exit_requested[0]:\n",
    "        polygon_array = np.array(roi_points)\n",
    "        np.save(\"roi_points.npy\", polygon_array)\n",
    "        print(\"ROI points saved to 'roi_points.npy' at\", os.path.abspath(\"roi_points.npy\"))\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to cancel anytime\n",
    "        print(\"ROI selection canceled.\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ROI points: [[500, 14], [857, 4], [1271, 398], [102, 419]]\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 85.2ms\n",
      "Speed: 4.8ms preprocess, 85.2ms inference, 12.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Vehicle COCO class IDs: car, motorcycle, bus, truck, auto (auto = rickshaw not in COCO, optional)\n",
    "VEHICLE_CLASSES = [2, 3, 5, 7]  # 2: car, 3: motorcycle, 5: bus, 7: truck\n",
    "\n",
    "# --- Load ROI from saved file ---\n",
    "try:\n",
    "    roi_polygon = np.load(\"roi_points.npy\")\n",
    "    roi_polygon = roi_polygon.tolist()  # Convert to list of tuples\n",
    "    print(f\"Loaded ROI points: {roi_polygon}\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load ROI points. Make sure roi_points.npy exists.\")\n",
    "    exit()\n",
    "\n",
    "# Helper function\n",
    "def is_inside_polygon(point, polygon):\n",
    "    return cv2.pointPolygonTest(np.array(polygon, dtype=np.int32), point, False) >= 0\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture(\"traffic testvideo.mp4\")\n",
    "ret, first_frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Failed to read video.\")\n",
    "    exit()\n",
    "\n",
    "# Tracking setup\n",
    "visible_ids = set()\n",
    "current_count = 0\n",
    "\n",
    "# --- Main Loop ---\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOv8 tracking\n",
    "    results = model.track(frame, persist=True, tracker=\"bytetrack.yaml\")[0]\n",
    "\n",
    "    output_frame = frame.copy()\n",
    "    current_ids = set()\n",
    "\n",
    "    if results.boxes.id is not None:\n",
    "        ids = results.boxes.id.cpu().numpy()\n",
    "        boxes = results.boxes.xyxy.cpu().numpy()\n",
    "        class_ids = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "        for box, track_id, cls in zip(boxes, ids, class_ids):\n",
    "            cls = int(cls)\n",
    "            track_id = int(track_id)\n",
    "            if cls not in VEHICLE_CLASSES:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "            if is_inside_polygon(center, roi_polygon):\n",
    "                current_ids.add(track_id)\n",
    "\n",
    "                # Draw box and center\n",
    "                cv2.rectangle(output_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(output_frame, f'ID: {track_id}', (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "                cv2.circle(output_frame, center, 5, (255, 0, 0), -1)\n",
    "\n",
    "    # Count logic\n",
    "    new_ids = current_ids - visible_ids\n",
    "    exited_ids = visible_ids - current_ids\n",
    "    current_count += len(new_ids) - len(exited_ids)\n",
    "    visible_ids = current_ids\n",
    "\n",
    "    # Draw ROI\n",
    "    cv2.polylines(output_frame, [np.array(roi_polygon, dtype=np.int32)], True, (0, 255, 255), 2)\n",
    "    cv2.putText(output_frame, f\"Vehicles in ROI: {current_count}\", (30, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "    # Display\n",
    "    cv2.imshow(\"YOLOv8 ROI Vehicle Counter\", output_frame)\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e238fab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MidasNet_small(\n",
       "  (pretrained): Module(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2dSameExport(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "      (3): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pw): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2dSameExport(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2dSameExport(192, 192, kernel_size=(5, 5), stride=(2, 2), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2dSameExport(288, 288, kernel_size=(3, 3), stride=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2dSameExport(816, 816, kernel_size=(5, 5), stride=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (scratch): Module(\n",
       "    (layer1_rn): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer2_rn): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer3_rn): Conv2d(136, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer4_rn): Conv2d(384, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (activation): ReLU()\n",
       "    (refinenet4): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet3): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet2): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet1): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (output_conv): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Interpolate()\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MiDaS model\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99631516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MiDaS model and transforms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n",
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing video... Press 'q' to quit.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m midas_transforms(input_image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 71\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmidas\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m     73\u001b[0m         prediction\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     74\u001b[0m         size\u001b[38;5;241m=\u001b[39mframe\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     75\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m         align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     )\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# === Normalize Depth Map ===\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/intel-isl_MiDaS_master/midas/midas_net_custom.py:89\u001b[0m, in \u001b[0;36mMidasNet_small.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m layer_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m     88\u001b[0m layer_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mlayer2(layer_1)\n\u001b[0;32m---> 89\u001b[0m layer_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m layer_4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mlayer4(layer_3)\n\u001b[1;32m     92\u001b[0m layer_1_rn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratch\u001b[38;5;241m.\u001b[39mlayer1_rn(layer_1)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master/geffnet/efficientnet_builder.py:237\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Depth-wise convolution\u001b[39;00m\n\u001b[1;32m    236\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_dw(x)\n\u001b[0;32m--> 237\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(x)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Squeeze-and-excitation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# midas_with_augmentation.py\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==================== MiDaS Setup ====================\n",
    "print(\"[INFO] Loading MiDaS model and transforms...\")\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# ==================== Load ROI ====================\n",
    "roi_file = \"roi_points.npy\"\n",
    "if not os.path.exists(roi_file):\n",
    "    print(\"ROI file not found.\")\n",
    "    exit()\n",
    "\n",
    "roi_points = np.load(roi_file)\n",
    "roi_polygon = np.array(roi_points, dtype=np.int32)\n",
    "\n",
    "# ==================== Augmentation Function ====================\n",
    "def enhance_frame(frame):\n",
    "    # --- Bilateral filter for denoising while preserving edges ---\n",
    "    smooth = cv2.bilateralFilter(frame, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # --- Sobel edge detection ---\n",
    "    gray = cv2.cvtColor(smooth, cv2.COLOR_BGR2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    sobel_edges = cv2.magnitude(sobelx, sobely)\n",
    "    sobel_edges = np.clip(sobel_edges, 0, 255).astype(np.uint8)\n",
    "    sobel_colored = cv2.cvtColor(sobel_edges, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # --- Fuse edge map and smooth image ---\n",
    "    fused = cv2.addWeighted(smooth, 0.8, sobel_colored, 0.2, 0)\n",
    "\n",
    "    # --- Sharpening ---\n",
    "    sharpen_kernel = np.array([[0, -1, 0],\n",
    "                               [-1,  5, -1],\n",
    "                               [0, -1, 0]])\n",
    "    final = cv2.filter2D(fused, -1, sharpen_kernel)\n",
    "\n",
    "    return final\n",
    "\n",
    "# ==================== Video Stream ====================\n",
    "cap = cv2.VideoCapture(\"traffic testvideo.mp4\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Failed to open video.\")\n",
    "    exit()\n",
    "\n",
    "print(\"[INFO] Processing video... Press 'q' to quit.\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # === Optional Resize to Improve Performance ===\n",
    "    frame = cv2.resize(frame, (1280, 720))  # You can comment this out if needed\n",
    "\n",
    "    # === Apply Augmentation ===\n",
    "    enhanced_frame = enhance_frame(frame)\n",
    "\n",
    "    # === MiDaS Depth Estimation ===\n",
    "    input_image = cv2.cvtColor(enhanced_frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = midas_transforms(input_image).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_tensor)\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=frame.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False\n",
    "        ).squeeze()\n",
    "\n",
    "    # === Normalize Depth Map ===\n",
    "    depth_map = prediction.cpu().numpy()\n",
    "    depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # === Mask ROI ===\n",
    "    mask = np.zeros_like(depth_map_normalized)\n",
    "    cv2.fillPoly(mask, [roi_polygon], 255)\n",
    "    masked_depth = cv2.bitwise_and(depth_map_normalized, mask)\n",
    "\n",
    "    # === Depth Histogram-Based Queue Estimation ===\n",
    "    nonzero_depth = masked_depth[masked_depth > 0]\n",
    "\n",
    "    if nonzero_depth.size > 0:\n",
    "        # Histogram bins: Close (0-85), Medium (86-170), Far (171-255)\n",
    "        hist, bin_edges = np.histogram(nonzero_depth, bins=[0, 85, 170, 256])\n",
    "        close_count, medium_count, far_count = hist\n",
    "        total = close_count + medium_count + far_count\n",
    "        queue_ratio = close_count / total if total > 0 else 0\n",
    "        queue_length = queue_ratio * 100  # Percentage indicator of queue length\n",
    "    else:\n",
    "        queue_length = 0\n",
    "\n",
    "    # === Overlay Display ===\n",
    "    overlay = cv2.cvtColor(masked_depth, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.polylines(overlay, [roi_polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "    cv2.putText(overlay, f\"Queue Estimation: {queue_length:.2f}%\", (30, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    # === Show Output ===\n",
    "    cv2.imshow(\"ROI Depth & Queue Estimation with Augmentation\", overlay)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ==================== Cleanup ====================\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ca8ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n",
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 cars, 1 truck, 88.9ms\n",
      "Speed: 4.2ms preprocess, 88.9ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 18.4ms\n",
      "Speed: 1.8ms preprocess, 18.4ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 bus, 1 truck, 17.7ms\n",
      "Speed: 2.1ms preprocess, 17.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 bus, 1 truck, 19.9ms\n",
      "Speed: 1.9ms preprocess, 19.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 21.6ms\n",
      "Speed: 2.9ms preprocess, 21.6ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 16.4ms\n",
      "Speed: 2.2ms preprocess, 16.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 22.1ms\n",
      "Speed: 1.9ms preprocess, 22.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 25.5ms\n",
      "Speed: 3.1ms preprocess, 25.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 23.6ms\n",
      "Speed: 2.9ms preprocess, 23.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 truck, 19.5ms\n",
      "Speed: 3.5ms preprocess, 19.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 21.2ms\n",
      "Speed: 1.8ms preprocess, 21.2ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 30.3ms\n",
      "Speed: 2.7ms preprocess, 30.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 22.7ms\n",
      "Speed: 2.1ms preprocess, 22.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 28.5ms\n",
      "Speed: 2.7ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 27.5ms\n",
      "Speed: 2.8ms preprocess, 27.5ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 truck, 17.4ms\n",
      "Speed: 2.1ms preprocess, 17.4ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 27.1ms\n",
      "Speed: 3.0ms preprocess, 27.1ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 30.8ms\n",
      "Speed: 3.0ms preprocess, 30.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 28.3ms\n",
      "Speed: 2.4ms preprocess, 28.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 34.2ms\n",
      "Speed: 2.2ms preprocess, 34.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 28.5ms\n",
      "Speed: 2.2ms preprocess, 28.5ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 14.5ms\n",
      "Speed: 2.5ms preprocess, 14.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 31.8ms\n",
      "Speed: 3.4ms preprocess, 31.8ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 27.6ms\n",
      "Speed: 2.1ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 36.9ms\n",
      "Speed: 3.2ms preprocess, 36.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 26.5ms\n",
      "Speed: 2.3ms preprocess, 26.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 24.0ms\n",
      "Speed: 1.9ms preprocess, 24.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 train, 32.2ms\n",
      "Speed: 2.2ms preprocess, 32.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 train, 11.4ms\n",
      "Speed: 2.7ms preprocess, 11.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 26.7ms\n",
      "Speed: 2.0ms preprocess, 26.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 27.1ms\n",
      "Speed: 2.2ms preprocess, 27.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 22.5ms\n",
      "Speed: 2.0ms preprocess, 22.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 30.2ms\n",
      "Speed: 3.5ms preprocess, 30.2ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 22.8ms\n",
      "Speed: 2.4ms preprocess, 22.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 23.0ms\n",
      "Speed: 3.2ms preprocess, 23.0ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 22.9ms\n",
      "Speed: 2.9ms preprocess, 22.9ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.9ms\n",
      "Speed: 2.3ms preprocess, 22.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.0ms\n",
      "Speed: 2.3ms preprocess, 24.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 17.6ms\n",
      "Speed: 2.1ms preprocess, 17.6ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 20.6ms\n",
      "Speed: 2.9ms preprocess, 20.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 20.0ms\n",
      "Speed: 2.3ms preprocess, 20.0ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.7ms\n",
      "Speed: 2.8ms preprocess, 30.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.2ms\n",
      "Speed: 3.9ms preprocess, 28.2ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 18.5ms\n",
      "Speed: 2.2ms preprocess, 18.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 17.4ms\n",
      "Speed: 2.4ms preprocess, 17.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.7ms\n",
      "Speed: 2.9ms preprocess, 28.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.0ms\n",
      "Speed: 2.2ms preprocess, 22.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.6ms\n",
      "Speed: 3.6ms preprocess, 23.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.3ms\n",
      "Speed: 4.9ms preprocess, 27.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 17.7ms\n",
      "Speed: 2.2ms preprocess, 17.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.1ms\n",
      "Speed: 3.5ms preprocess, 29.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 19.5ms\n",
      "Speed: 2.3ms preprocess, 19.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.8ms\n",
      "Speed: 3.5ms preprocess, 27.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 31.6ms\n",
      "Speed: 3.6ms preprocess, 31.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.4ms\n",
      "Speed: 3.1ms preprocess, 29.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.8ms\n",
      "Speed: 2.7ms preprocess, 26.8ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 27.3ms\n",
      "Speed: 2.9ms preprocess, 27.3ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 3.3ms preprocess, 40.0ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 33.2ms\n",
      "Speed: 3.1ms preprocess, 33.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.6ms\n",
      "Speed: 3.2ms preprocess, 26.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.1ms\n",
      "Speed: 3.3ms preprocess, 29.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 18.7ms\n",
      "Speed: 3.3ms preprocess, 18.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 21.3ms\n",
      "Speed: 2.3ms preprocess, 21.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 3.1ms preprocess, 42.3ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.3ms\n",
      "Speed: 2.0ms preprocess, 25.3ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 3.0ms preprocess, 38.9ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.1ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.8ms\n",
      "Speed: 2.7ms preprocess, 26.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 3.9ms preprocess, 39.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 16.4ms\n",
      "Speed: 2.1ms preprocess, 16.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.8ms\n",
      "Speed: 3.4ms preprocess, 25.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 30.6ms\n",
      "Speed: 2.9ms preprocess, 30.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 29.3ms\n",
      "Speed: 3.2ms preprocess, 29.3ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 32.4ms\n",
      "Speed: 2.7ms preprocess, 32.4ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 20.9ms\n",
      "Speed: 3.2ms preprocess, 20.9ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.4ms preprocess, 13.7ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.2ms\n",
      "Speed: 2.6ms preprocess, 23.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 3.3ms preprocess, 37.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.3ms\n",
      "Speed: 2.4ms preprocess, 24.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.9ms\n",
      "Speed: 2.6ms preprocess, 23.9ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.1ms\n",
      "Speed: 2.6ms preprocess, 28.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 2.8ms preprocess, 37.8ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.0ms\n",
      "Speed: 2.8ms preprocess, 26.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 26.5ms\n",
      "Speed: 3.5ms preprocess, 26.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 21.6ms\n",
      "Speed: 2.8ms preprocess, 21.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 32.0ms\n",
      "Speed: 2.7ms preprocess, 32.0ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.2ms\n",
      "Speed: 2.2ms preprocess, 23.2ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 46.6ms\n",
      "Speed: 3.1ms preprocess, 46.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 3.6ms preprocess, 36.7ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 2.8ms preprocess, 40.7ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 2.6ms preprocess, 15.2ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 3.0ms preprocess, 36.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 2.1ms preprocess, 14.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.2ms preprocess, 13.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 3.2ms preprocess, 35.2ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.5ms\n",
      "Speed: 2.6ms preprocess, 23.5ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 3.4ms preprocess, 41.4ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 3.4ms preprocess, 39.2ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 2.6ms preprocess, 14.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 23.1ms\n",
      "Speed: 2.7ms preprocess, 23.1ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 18.7ms\n",
      "Speed: 2.2ms preprocess, 18.7ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 15.3ms\n",
      "Speed: 2.1ms preprocess, 15.3ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 33.3ms\n",
      "Speed: 3.0ms preprocess, 33.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 25.5ms\n",
      "Speed: 3.4ms preprocess, 25.5ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 40.8ms\n",
      "Speed: 3.7ms preprocess, 40.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 23.3ms\n",
      "Speed: 2.3ms preprocess, 23.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 22.9ms\n",
      "Speed: 2.1ms preprocess, 22.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 22.0ms\n",
      "Speed: 2.6ms preprocess, 22.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 28.1ms\n",
      "Speed: 2.5ms preprocess, 28.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 26.8ms\n",
      "Speed: 2.4ms preprocess, 26.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 44.9ms\n",
      "Speed: 3.0ms preprocess, 44.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 25.8ms\n",
      "Speed: 2.1ms preprocess, 25.8ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 38.7ms\n",
      "Speed: 3.4ms preprocess, 38.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 1 truck, 24.1ms\n",
      "Speed: 2.1ms preprocess, 24.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 cars, 1 truck, 32.1ms\n",
      "Speed: 3.1ms preprocess, 32.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 cars, 1 truck, 31.4ms\n",
      "Speed: 3.5ms preprocess, 31.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 cars, 1 truck, 21.2ms\n",
      "Speed: 2.0ms preprocess, 21.2ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 3 cars, 19.0ms\n",
      "Speed: 2.2ms preprocess, 19.0ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 cars, 25.8ms\n",
      "Speed: 2.3ms preprocess, 25.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 4 cars, 18.6ms\n",
      "Speed: 2.1ms preprocess, 18.6ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 4 cars, 18.9ms\n",
      "Speed: 2.4ms preprocess, 18.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 4 cars, 38.5ms\n",
      "Speed: 3.3ms preprocess, 38.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 4 cars, 34.7ms\n",
      "Speed: 3.4ms preprocess, 34.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 4 cars, 18.4ms\n",
      "Speed: 2.6ms preprocess, 18.4ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 5 cars, 36.1ms\n",
      "Speed: 3.0ms preprocess, 36.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 5 cars, 31.4ms\n",
      "Speed: 3.6ms preprocess, 31.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 5 cars, 32.1ms\n",
      "Speed: 2.0ms preprocess, 32.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 28.2ms\n",
      "Speed: 3.0ms preprocess, 28.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 27.0ms\n",
      "Speed: 2.7ms preprocess, 27.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 34.0ms\n",
      "Speed: 2.3ms preprocess, 34.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 16.5ms\n",
      "Speed: 2.1ms preprocess, 16.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 29.4ms\n",
      "Speed: 3.7ms preprocess, 29.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 1 truck, 34.5ms\n",
      "Speed: 2.6ms preprocess, 34.5ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 1 truck, 30.8ms\n",
      "Speed: 3.3ms preprocess, 30.8ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 16.2ms\n",
      "Speed: 2.0ms preprocess, 16.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 33.2ms\n",
      "Speed: 3.7ms preprocess, 33.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 19.1ms\n",
      "Speed: 2.4ms preprocess, 19.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 14.7ms\n",
      "Speed: 1.9ms preprocess, 14.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 19.3ms\n",
      "Speed: 2.7ms preprocess, 19.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 31.0ms\n",
      "Speed: 1.7ms preprocess, 31.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 29.8ms\n",
      "Speed: 3.2ms preprocess, 29.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 32.5ms\n",
      "Speed: 3.9ms preprocess, 32.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 30.3ms\n",
      "Speed: 4.1ms preprocess, 30.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 25.4ms\n",
      "Speed: 2.6ms preprocess, 25.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 24.6ms\n",
      "Speed: 2.6ms preprocess, 24.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 29.2ms\n",
      "Speed: 2.1ms preprocess, 29.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 23.1ms\n",
      "Speed: 2.2ms preprocess, 23.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 30.4ms\n",
      "Speed: 2.5ms preprocess, 30.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 20.7ms\n",
      "Speed: 2.7ms preprocess, 20.7ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 28.8ms\n",
      "Speed: 3.1ms preprocess, 28.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 17.1ms\n",
      "Speed: 2.1ms preprocess, 17.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 22.2ms\n",
      "Speed: 2.3ms preprocess, 22.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 28.6ms\n",
      "Speed: 2.6ms preprocess, 28.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 14.6ms\n",
      "Speed: 2.7ms preprocess, 14.6ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 15.1ms\n",
      "Speed: 2.0ms preprocess, 15.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 28.4ms\n",
      "Speed: 2.2ms preprocess, 28.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 37.9ms\n",
      "Speed: 3.2ms preprocess, 37.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 31.4ms\n",
      "Speed: 2.0ms preprocess, 31.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 17.8ms\n",
      "Speed: 3.7ms preprocess, 17.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 28.8ms\n",
      "Speed: 3.8ms preprocess, 28.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 21.2ms\n",
      "Speed: 2.3ms preprocess, 21.2ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 24.4ms\n",
      "Speed: 2.4ms preprocess, 24.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 23.6ms\n",
      "Speed: 2.3ms preprocess, 23.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 14.7ms\n",
      "Speed: 2.9ms preprocess, 14.7ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 16.0ms\n",
      "Speed: 2.2ms preprocess, 16.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 35.9ms\n",
      "Speed: 2.7ms preprocess, 35.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 22.6ms\n",
      "Speed: 2.2ms preprocess, 22.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 bus, 17.8ms\n",
      "Speed: 2.8ms preprocess, 17.8ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 bus, 32.8ms\n",
      "Speed: 2.9ms preprocess, 32.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 truck, 25.1ms\n",
      "Speed: 2.6ms preprocess, 25.1ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 31.6ms\n",
      "Speed: 3.2ms preprocess, 31.6ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 1 truck, 30.5ms\n",
      "Speed: 2.9ms preprocess, 30.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 28.6ms\n",
      "Speed: 2.6ms preprocess, 28.6ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 29.9ms\n",
      "Speed: 3.3ms preprocess, 29.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 39.8ms\n",
      "Speed: 2.6ms preprocess, 39.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 12.2ms\n",
      "Speed: 2.3ms preprocess, 12.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 28.0ms\n",
      "Speed: 2.4ms preprocess, 28.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 23.9ms\n",
      "Speed: 2.1ms preprocess, 23.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6 cars, 1 truck, 25.3ms\n",
      "Speed: 3.0ms preprocess, 25.3ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 32.6ms\n",
      "Speed: 2.9ms preprocess, 32.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 50.1ms\n",
      "Speed: 4.4ms preprocess, 50.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 15.3ms\n",
      "Speed: 2.2ms preprocess, 15.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 19.7ms\n",
      "Speed: 2.1ms preprocess, 19.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 25.4ms\n",
      "Speed: 2.1ms preprocess, 25.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 27.5ms\n",
      "Speed: 2.5ms preprocess, 27.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 19.2ms\n",
      "Speed: 2.4ms preprocess, 19.2ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 34.2ms\n",
      "Speed: 3.1ms preprocess, 34.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 38.6ms\n",
      "Speed: 3.8ms preprocess, 38.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 22.2ms\n",
      "Speed: 2.0ms preprocess, 22.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 38.5ms\n",
      "Speed: 3.0ms preprocess, 38.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 35.3ms\n",
      "Speed: 3.1ms preprocess, 35.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 35.1ms\n",
      "Speed: 2.0ms preprocess, 35.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 41.4ms\n",
      "Speed: 3.1ms preprocess, 41.4ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 33.7ms\n",
      "Speed: 3.2ms preprocess, 33.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 31.1ms\n",
      "Speed: 2.0ms preprocess, 31.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 36.3ms\n",
      "Speed: 3.1ms preprocess, 36.3ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 13.3ms\n",
      "Speed: 2.2ms preprocess, 13.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 28.5ms\n",
      "Speed: 3.1ms preprocess, 28.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 21.0ms\n",
      "Speed: 2.3ms preprocess, 21.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 25.6ms\n",
      "Speed: 2.5ms preprocess, 25.6ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 22.7ms\n",
      "Speed: 2.1ms preprocess, 22.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 29.5ms\n",
      "Speed: 2.7ms preprocess, 29.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 25.1ms\n",
      "Speed: 3.0ms preprocess, 25.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 29.8ms\n",
      "Speed: 2.4ms preprocess, 29.8ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 22.1ms\n",
      "Speed: 2.4ms preprocess, 22.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 2 trucks, 15.5ms\n",
      "Speed: 2.3ms preprocess, 15.5ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 1 truck, 35.3ms\n",
      "Speed: 3.4ms preprocess, 35.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 1 truck, 16.9ms\n",
      "Speed: 2.1ms preprocess, 16.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 1 truck, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 33.5ms\n",
      "Speed: 3.2ms preprocess, 33.5ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 27.6ms\n",
      "Speed: 2.3ms preprocess, 27.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 1 truck, 22.1ms\n",
      "Speed: 2.4ms preprocess, 22.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 truck, 19.7ms\n",
      "Speed: 2.3ms preprocess, 19.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 truck, 31.8ms\n",
      "Speed: 2.4ms preprocess, 31.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 21.6ms\n",
      "Speed: 2.6ms preprocess, 21.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 31.8ms\n",
      "Speed: 2.3ms preprocess, 31.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 18.9ms\n",
      "Speed: 2.2ms preprocess, 18.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 35.3ms\n",
      "Speed: 2.7ms preprocess, 35.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 25.4ms\n",
      "Speed: 3.3ms preprocess, 25.4ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 23.2ms\n",
      "Speed: 2.1ms preprocess, 23.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 22.9ms\n",
      "Speed: 2.1ms preprocess, 22.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 24.8ms\n",
      "Speed: 2.2ms preprocess, 24.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 23.0ms\n",
      "Speed: 2.7ms preprocess, 23.0ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 34.3ms\n",
      "Speed: 3.1ms preprocess, 34.3ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 35.0ms\n",
      "Speed: 3.0ms preprocess, 35.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 7.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 31.5ms\n",
      "Speed: 3.1ms preprocess, 31.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 12.3ms\n",
      "Speed: 2.2ms preprocess, 12.3ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 30.5ms\n",
      "Speed: 2.4ms preprocess, 30.5ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 26.9ms\n",
      "Speed: 3.2ms preprocess, 26.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 20.5ms\n",
      "Speed: 2.0ms preprocess, 20.5ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 27.8ms\n",
      "Speed: 3.0ms preprocess, 27.8ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 32.6ms\n",
      "Speed: 3.0ms preprocess, 32.6ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 27.9ms\n",
      "Speed: 2.5ms preprocess, 27.9ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 33.1ms\n",
      "Speed: 2.9ms preprocess, 33.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 12.8ms\n",
      "Speed: 2.6ms preprocess, 12.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 16.4ms\n",
      "Speed: 2.2ms preprocess, 16.4ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 bus, 1 truck, 23.2ms\n",
      "Speed: 2.6ms preprocess, 23.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 26.4ms\n",
      "Speed: 1.8ms preprocess, 26.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 17.8ms\n",
      "Speed: 2.7ms preprocess, 17.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 20.1ms\n",
      "Speed: 2.0ms preprocess, 20.1ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 47.3ms\n",
      "Speed: 3.7ms preprocess, 47.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 38.5ms\n",
      "Speed: 2.2ms preprocess, 38.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 35.7ms\n",
      "Speed: 2.4ms preprocess, 35.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 13.8ms\n",
      "Speed: 2.2ms preprocess, 13.8ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 14.4ms\n",
      "Speed: 2.0ms preprocess, 14.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 24.8ms\n",
      "Speed: 2.2ms preprocess, 24.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 28.6ms\n",
      "Speed: 2.0ms preprocess, 28.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 30.3ms\n",
      "Speed: 2.1ms preprocess, 30.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 28.0ms\n",
      "Speed: 2.8ms preprocess, 28.0ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 20.6ms\n",
      "Speed: 2.4ms preprocess, 20.6ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 19.5ms\n",
      "Speed: 2.3ms preprocess, 19.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 28.2ms\n",
      "Speed: 2.9ms preprocess, 28.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 15.9ms\n",
      "Speed: 2.0ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 19.2ms\n",
      "Speed: 2.1ms preprocess, 19.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 34.2ms\n",
      "Speed: 2.8ms preprocess, 34.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 31.8ms\n",
      "Speed: 3.1ms preprocess, 31.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 41.9ms\n",
      "Speed: 3.5ms preprocess, 41.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 23.9ms\n",
      "Speed: 3.6ms preprocess, 23.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 24.1ms\n",
      "Speed: 2.6ms preprocess, 24.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 36.8ms\n",
      "Speed: 2.2ms preprocess, 36.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 26.7ms\n",
      "Speed: 2.5ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 24.8ms\n",
      "Speed: 2.7ms preprocess, 24.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 16.3ms\n",
      "Speed: 2.9ms preprocess, 16.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 33.7ms\n",
      "Speed: 3.6ms preprocess, 33.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 29.1ms\n",
      "Speed: 1.9ms preprocess, 29.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 14.3ms\n",
      "Speed: 2.1ms preprocess, 14.3ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 22.7ms\n",
      "Speed: 3.1ms preprocess, 22.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 12.3ms\n",
      "Speed: 1.9ms preprocess, 12.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 26.3ms\n",
      "Speed: 3.6ms preprocess, 26.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 19.7ms\n",
      "Speed: 2.0ms preprocess, 19.7ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 32.9ms\n",
      "Speed: 2.6ms preprocess, 32.9ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 30.0ms\n",
      "Speed: 2.9ms preprocess, 30.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 25.0ms\n",
      "Speed: 3.7ms preprocess, 25.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 35.7ms\n",
      "Speed: 2.9ms preprocess, 35.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 17.4ms\n",
      "Speed: 2.2ms preprocess, 17.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 30.4ms\n",
      "Speed: 3.2ms preprocess, 30.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 38.0ms\n",
      "Speed: 3.9ms preprocess, 38.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 30.9ms\n",
      "Speed: 2.9ms preprocess, 30.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 31.8ms\n",
      "Speed: 3.1ms preprocess, 31.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 43.0ms\n",
      "Speed: 3.4ms preprocess, 43.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 12.9ms\n",
      "Speed: 1.8ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 34.0ms\n",
      "Speed: 2.3ms preprocess, 34.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 17.7ms\n",
      "Speed: 2.2ms preprocess, 17.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 23.4ms\n",
      "Speed: 2.5ms preprocess, 23.4ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 38.6ms\n",
      "Speed: 2.4ms preprocess, 38.6ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 bus, 36.4ms\n",
      "Speed: 2.7ms preprocess, 36.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 24.7ms\n",
      "Speed: 2.7ms preprocess, 24.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 37.4ms\n",
      "Speed: 3.8ms preprocess, 37.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 21.1ms\n",
      "Speed: 2.1ms preprocess, 21.1ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 12.2ms\n",
      "Speed: 2.2ms preprocess, 12.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 22.2ms\n",
      "Speed: 2.2ms preprocess, 22.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 28.6ms\n",
      "Speed: 3.4ms preprocess, 28.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 45.4ms\n",
      "Speed: 3.0ms preprocess, 45.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 20.2ms\n",
      "Speed: 2.1ms preprocess, 20.2ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 16.1ms\n",
      "Speed: 2.1ms preprocess, 16.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 35.7ms\n",
      "Speed: 2.9ms preprocess, 35.7ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 39.9ms\n",
      "Speed: 2.8ms preprocess, 39.9ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 33.1ms\n",
      "Speed: 3.2ms preprocess, 33.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 33.7ms\n",
      "Speed: 2.6ms preprocess, 33.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 32.6ms\n",
      "Speed: 2.3ms preprocess, 32.6ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 19.6ms\n",
      "Speed: 2.2ms preprocess, 19.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 45.8ms\n",
      "Speed: 2.4ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 43.6ms\n",
      "Speed: 2.3ms preprocess, 43.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 12.9ms\n",
      "Speed: 2.0ms preprocess, 12.9ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 27.0ms\n",
      "Speed: 2.0ms preprocess, 27.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 36.5ms\n",
      "Speed: 3.3ms preprocess, 36.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 28.0ms\n",
      "Speed: 2.9ms preprocess, 28.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 19.6ms\n",
      "Speed: 2.0ms preprocess, 19.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 23.6ms\n",
      "Speed: 2.5ms preprocess, 23.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 24.8ms\n",
      "Speed: 2.3ms preprocess, 24.8ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 22.1ms\n",
      "Speed: 2.0ms preprocess, 22.1ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 20.6ms\n",
      "Speed: 2.2ms preprocess, 20.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 39.1ms\n",
      "Speed: 5.7ms preprocess, 39.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 11.7ms\n",
      "Speed: 2.1ms preprocess, 11.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 31.4ms\n",
      "Speed: 2.1ms preprocess, 31.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 18.9ms\n",
      "Speed: 2.0ms preprocess, 18.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 30.4ms\n",
      "Speed: 3.1ms preprocess, 30.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 46.9ms\n",
      "Speed: 2.6ms preprocess, 46.9ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 33.4ms\n",
      "Speed: 3.2ms preprocess, 33.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 39.6ms\n",
      "Speed: 3.3ms preprocess, 39.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 42.5ms\n",
      "Speed: 3.1ms preprocess, 42.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 36.6ms\n",
      "Speed: 2.8ms preprocess, 36.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 15.8ms\n",
      "Speed: 2.2ms preprocess, 15.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 28.5ms\n",
      "Speed: 2.5ms preprocess, 28.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 16.0ms\n",
      "Speed: 2.2ms preprocess, 16.0ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 38.9ms\n",
      "Speed: 2.5ms preprocess, 38.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 17.2ms\n",
      "Speed: 2.3ms preprocess, 17.2ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 18.3ms\n",
      "Speed: 2.3ms preprocess, 18.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 35.8ms\n",
      "Speed: 3.6ms preprocess, 35.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 35.4ms\n",
      "Speed: 3.0ms preprocess, 35.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 17.9ms\n",
      "Speed: 1.9ms preprocess, 17.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 34.1ms\n",
      "Speed: 3.5ms preprocess, 34.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 38.5ms\n",
      "Speed: 3.6ms preprocess, 38.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 51.8ms\n",
      "Speed: 3.3ms preprocess, 51.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 27.4ms\n",
      "Speed: 2.5ms preprocess, 27.4ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 33.2ms\n",
      "Speed: 3.1ms preprocess, 33.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 32.5ms\n",
      "Speed: 3.2ms preprocess, 32.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 23.7ms\n",
      "Speed: 2.6ms preprocess, 23.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 23.8ms\n",
      "Speed: 2.7ms preprocess, 23.8ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 33.8ms\n",
      "Speed: 3.1ms preprocess, 33.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 38.1ms\n",
      "Speed: 3.2ms preprocess, 38.1ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 31.7ms\n",
      "Speed: 2.5ms preprocess, 31.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 28.7ms\n",
      "Speed: 2.3ms preprocess, 28.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 31.3ms\n",
      "Speed: 2.1ms preprocess, 31.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 14.4ms\n",
      "Speed: 2.0ms preprocess, 14.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 24.3ms\n",
      "Speed: 3.0ms preprocess, 24.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 38.0ms\n",
      "Speed: 3.1ms preprocess, 38.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 25.5ms\n",
      "Speed: 3.4ms preprocess, 25.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 37.7ms\n",
      "Speed: 3.2ms preprocess, 37.7ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 38.4ms\n",
      "Speed: 2.3ms preprocess, 38.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 38.2ms\n",
      "Speed: 3.2ms preprocess, 38.2ms inference, 8.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 37.1ms\n",
      "Speed: 3.2ms preprocess, 37.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 22.8ms\n",
      "Speed: 3.5ms preprocess, 22.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 22.4ms\n",
      "Speed: 2.4ms preprocess, 22.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 26.6ms\n",
      "Speed: 2.6ms preprocess, 26.6ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 19.6ms\n",
      "Speed: 3.5ms preprocess, 19.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 25.8ms\n",
      "Speed: 3.2ms preprocess, 25.8ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 19.2ms\n",
      "Speed: 2.2ms preprocess, 19.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 17.9ms\n",
      "Speed: 2.0ms preprocess, 17.9ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 12.8ms\n",
      "Speed: 2.4ms preprocess, 12.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 14.3ms\n",
      "Speed: 2.4ms preprocess, 14.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 train, 30.6ms\n",
      "Speed: 2.6ms preprocess, 30.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 train, 38.3ms\n",
      "Speed: 2.8ms preprocess, 38.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 1 train, 35.0ms\n",
      "Speed: 2.6ms preprocess, 35.0ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 21.6ms\n",
      "Speed: 3.1ms preprocess, 21.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 41.2ms\n",
      "Speed: 3.2ms preprocess, 41.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 18.9ms\n",
      "Speed: 2.5ms preprocess, 18.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 30.6ms\n",
      "Speed: 3.4ms preprocess, 30.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 38.1ms\n",
      "Speed: 3.2ms preprocess, 38.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 47.8ms\n",
      "Speed: 3.6ms preprocess, 47.8ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 19.6ms\n",
      "Speed: 2.1ms preprocess, 19.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 31.7ms\n",
      "Speed: 2.0ms preprocess, 31.7ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 22.9ms\n",
      "Speed: 2.0ms preprocess, 22.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 13.4ms\n",
      "Speed: 2.1ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 18.0ms\n",
      "Speed: 2.2ms preprocess, 18.0ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 30.9ms\n",
      "Speed: 2.2ms preprocess, 30.9ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 22.1ms\n",
      "Speed: 2.1ms preprocess, 22.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 29.9ms\n",
      "Speed: 2.8ms preprocess, 29.9ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 33.1ms\n",
      "Speed: 3.1ms preprocess, 33.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 32.3ms\n",
      "Speed: 3.1ms preprocess, 32.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 29.0ms\n",
      "Speed: 3.3ms preprocess, 29.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 25.7ms\n",
      "Speed: 2.6ms preprocess, 25.7ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 23.4ms\n",
      "Speed: 3.4ms preprocess, 23.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 33.5ms\n",
      "Speed: 3.0ms preprocess, 33.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 37.9ms\n",
      "Speed: 3.2ms preprocess, 37.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 36.4ms\n",
      "Speed: 3.1ms preprocess, 36.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 34.2ms\n",
      "Speed: 3.1ms preprocess, 34.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 24.0ms\n",
      "Speed: 2.8ms preprocess, 24.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 17.9ms\n",
      "Speed: 2.4ms preprocess, 17.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 23.5ms\n",
      "Speed: 2.6ms preprocess, 23.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 34.4ms\n",
      "Speed: 3.1ms preprocess, 34.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 31.5ms\n",
      "Speed: 3.0ms preprocess, 31.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 22.7ms\n",
      "Speed: 2.3ms preprocess, 22.7ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 41.5ms\n",
      "Speed: 2.6ms preprocess, 41.5ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 17.2ms\n",
      "Speed: 2.5ms preprocess, 17.2ms inference, 10.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 33.5ms\n",
      "Speed: 3.1ms preprocess, 33.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 23.7ms\n",
      "Speed: 2.4ms preprocess, 23.7ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 14.7ms\n",
      "Speed: 2.0ms preprocess, 14.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 29.2ms\n",
      "Speed: 2.3ms preprocess, 29.2ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 21.1ms\n",
      "Speed: 2.0ms preprocess, 21.1ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 33.7ms\n",
      "Speed: 2.4ms preprocess, 33.7ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 30.3ms\n",
      "Speed: 3.3ms preprocess, 30.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 29.0ms\n",
      "Speed: 3.4ms preprocess, 29.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 18.8ms\n",
      "Speed: 2.1ms preprocess, 18.8ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 14.1ms\n",
      "Speed: 1.9ms preprocess, 14.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 17.8ms\n",
      "Speed: 2.0ms preprocess, 17.8ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 31.7ms\n",
      "Speed: 2.0ms preprocess, 31.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 45.8ms\n",
      "Speed: 2.8ms preprocess, 45.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 26.1ms\n",
      "Speed: 2.7ms preprocess, 26.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 24.3ms\n",
      "Speed: 3.2ms preprocess, 24.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 34.3ms\n",
      "Speed: 2.9ms preprocess, 34.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 18.4ms\n",
      "Speed: 2.0ms preprocess, 18.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 32.0ms\n",
      "Speed: 3.2ms preprocess, 32.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 30.4ms\n",
      "Speed: 2.9ms preprocess, 30.4ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 29.2ms\n",
      "Speed: 3.6ms preprocess, 29.2ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 43.2ms\n",
      "Speed: 3.6ms preprocess, 43.2ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 21.5ms\n",
      "Speed: 2.1ms preprocess, 21.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 25.1ms\n",
      "Speed: 2.5ms preprocess, 25.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 19.3ms\n",
      "Speed: 2.4ms preprocess, 19.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 19.1ms\n",
      "Speed: 2.3ms preprocess, 19.1ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 bus, 26.6ms\n",
      "Speed: 2.1ms preprocess, 26.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 15 cars, 1 truck, 26.1ms\n",
      "Speed: 3.6ms preprocess, 26.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 1 bus, 1 truck, 35.3ms\n",
      "Speed: 3.1ms preprocess, 35.3ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 15 cars, 17.3ms\n",
      "Speed: 2.7ms preprocess, 17.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 15 cars, 24.6ms\n",
      "Speed: 1.9ms preprocess, 24.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 1 truck, 29.2ms\n",
      "Speed: 3.1ms preprocess, 29.2ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 26.3ms\n",
      "Speed: 2.5ms preprocess, 26.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 19.0ms\n",
      "Speed: 1.9ms preprocess, 19.0ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 21.0ms\n",
      "Speed: 2.2ms preprocess, 21.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 24.5ms\n",
      "Speed: 2.1ms preprocess, 24.5ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 27.1ms\n",
      "Speed: 2.3ms preprocess, 27.1ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 33.2ms\n",
      "Speed: 2.5ms preprocess, 33.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 31.3ms\n",
      "Speed: 2.7ms preprocess, 31.3ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 26.7ms\n",
      "Speed: 3.0ms preprocess, 26.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 37.2ms\n",
      "Speed: 3.4ms preprocess, 37.2ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 30.3ms\n",
      "Speed: 2.3ms preprocess, 30.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 25.2ms\n",
      "Speed: 2.3ms preprocess, 25.2ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 34.6ms\n",
      "Speed: 3.3ms preprocess, 34.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 19.9ms\n",
      "Speed: 2.4ms preprocess, 19.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 22.6ms\n",
      "Speed: 2.3ms preprocess, 22.6ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 15.3ms\n",
      "Speed: 2.2ms preprocess, 15.3ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 15.2ms\n",
      "Speed: 2.2ms preprocess, 15.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 33.9ms\n",
      "Speed: 3.2ms preprocess, 33.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 15.9ms\n",
      "Speed: 2.6ms preprocess, 15.9ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 33.3ms\n",
      "Speed: 3.8ms preprocess, 33.3ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 28.7ms\n",
      "Speed: 3.2ms preprocess, 28.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 24.5ms\n",
      "Speed: 2.3ms preprocess, 24.5ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 29.2ms\n",
      "Speed: 2.4ms preprocess, 29.2ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 19.2ms\n",
      "Speed: 2.0ms preprocess, 19.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 17.2ms\n",
      "Speed: 2.1ms preprocess, 17.2ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 42.5ms\n",
      "Speed: 3.4ms preprocess, 42.5ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 31.0ms\n",
      "Speed: 3.2ms preprocess, 31.0ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 44.9ms\n",
      "Speed: 2.8ms preprocess, 44.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 34.4ms\n",
      "Speed: 2.3ms preprocess, 34.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 27.6ms\n",
      "Speed: 2.2ms preprocess, 27.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 20.1ms\n",
      "Speed: 2.1ms preprocess, 20.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 14.5ms\n",
      "Speed: 1.9ms preprocess, 14.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 20.4ms\n",
      "Speed: 1.9ms preprocess, 20.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 29.1ms\n",
      "Speed: 3.0ms preprocess, 29.1ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 31.2ms\n",
      "Speed: 3.0ms preprocess, 31.2ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 31.8ms\n",
      "Speed: 2.7ms preprocess, 31.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 24.7ms\n",
      "Speed: 2.3ms preprocess, 24.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 27.7ms\n",
      "Speed: 2.1ms preprocess, 27.7ms inference, 10.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 12.4ms\n",
      "Speed: 2.2ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 41.5ms\n",
      "Speed: 3.1ms preprocess, 41.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 19.1ms\n",
      "Speed: 2.1ms preprocess, 19.1ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 39.8ms\n",
      "Speed: 3.3ms preprocess, 39.8ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 31.1ms\n",
      "Speed: 2.3ms preprocess, 31.1ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 37.6ms\n",
      "Speed: 3.8ms preprocess, 37.6ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 34.6ms\n",
      "Speed: 3.2ms preprocess, 34.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 28.9ms\n",
      "Speed: 2.8ms preprocess, 28.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 24.0ms\n",
      "Speed: 2.1ms preprocess, 24.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 36.0ms\n",
      "Speed: 3.8ms preprocess, 36.0ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 39.3ms\n",
      "Speed: 4.2ms preprocess, 39.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 37.0ms\n",
      "Speed: 3.9ms preprocess, 37.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 30.7ms\n",
      "Speed: 3.0ms preprocess, 30.7ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 31.5ms\n",
      "Speed: 2.3ms preprocess, 31.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 46.1ms\n",
      "Speed: 2.4ms preprocess, 46.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 16.5ms\n",
      "Speed: 2.8ms preprocess, 16.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 44.5ms\n",
      "Speed: 3.0ms preprocess, 44.5ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 34.6ms\n",
      "Speed: 3.2ms preprocess, 34.6ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 24.6ms\n",
      "Speed: 2.1ms preprocess, 24.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 43.6ms\n",
      "Speed: 3.4ms preprocess, 43.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 34.4ms\n",
      "Speed: 2.1ms preprocess, 34.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 27.3ms\n",
      "Speed: 3.1ms preprocess, 27.3ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 40.5ms\n",
      "Speed: 3.0ms preprocess, 40.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 26.4ms\n",
      "Speed: 2.2ms preprocess, 26.4ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14 cars, 14.1ms\n",
      "Speed: 2.1ms preprocess, 14.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 24.3ms\n",
      "Speed: 2.4ms preprocess, 24.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 30.5ms\n",
      "Speed: 3.6ms preprocess, 30.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 32.8ms\n",
      "Speed: 3.1ms preprocess, 32.8ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 27.6ms\n",
      "Speed: 2.6ms preprocess, 27.6ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 33.9ms\n",
      "Speed: 2.2ms preprocess, 33.9ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 37.8ms\n",
      "Speed: 3.2ms preprocess, 37.8ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 37.8ms\n",
      "Speed: 2.7ms preprocess, 37.8ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 29.4ms\n",
      "Speed: 3.2ms preprocess, 29.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 35.1ms\n",
      "Speed: 3.6ms preprocess, 35.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 31.3ms\n",
      "Speed: 2.4ms preprocess, 31.3ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 36.5ms\n",
      "Speed: 3.3ms preprocess, 36.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 33.2ms\n",
      "Speed: 2.8ms preprocess, 33.2ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 33.8ms\n",
      "Speed: 3.2ms preprocess, 33.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 31.0ms\n",
      "Speed: 2.9ms preprocess, 31.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 25.6ms\n",
      "Speed: 2.2ms preprocess, 25.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 14.3ms\n",
      "Speed: 2.0ms preprocess, 14.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 22.2ms\n",
      "Speed: 2.4ms preprocess, 22.2ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 37.8ms\n",
      "Speed: 3.3ms preprocess, 37.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 35.1ms\n",
      "Speed: 2.9ms preprocess, 35.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 24.7ms\n",
      "Speed: 2.3ms preprocess, 24.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 15.0ms\n",
      "Speed: 2.3ms preprocess, 15.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 14.3ms\n",
      "Speed: 2.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 36.6ms\n",
      "Speed: 3.3ms preprocess, 36.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 28.4ms\n",
      "Speed: 2.9ms preprocess, 28.4ms inference, 11.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 14.7ms\n",
      "Speed: 3.3ms preprocess, 14.7ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 43.3ms\n",
      "Speed: 3.4ms preprocess, 43.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 32.3ms\n",
      "Speed: 2.0ms preprocess, 32.3ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 32.6ms\n",
      "Speed: 2.3ms preprocess, 32.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 15.0ms\n",
      "Speed: 2.1ms preprocess, 15.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 23.3ms\n",
      "Speed: 2.1ms preprocess, 23.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 20.5ms\n",
      "Speed: 2.1ms preprocess, 20.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 34.4ms\n",
      "Speed: 3.1ms preprocess, 34.4ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 32.2ms\n",
      "Speed: 2.2ms preprocess, 32.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 40.3ms\n",
      "Speed: 3.6ms preprocess, 40.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 36.8ms\n",
      "Speed: 3.2ms preprocess, 36.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 18.9ms\n",
      "Speed: 1.9ms preprocess, 18.9ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 14.9ms\n",
      "Speed: 2.5ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 33.7ms\n",
      "Speed: 3.5ms preprocess, 33.7ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 37.0ms\n",
      "Speed: 2.7ms preprocess, 37.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 33.8ms\n",
      "Speed: 2.4ms preprocess, 33.8ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 27.9ms\n",
      "Speed: 2.1ms preprocess, 27.9ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 32.5ms\n",
      "Speed: 3.8ms preprocess, 32.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 37.1ms\n",
      "Speed: 3.3ms preprocess, 37.1ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 31.8ms\n",
      "Speed: 3.2ms preprocess, 31.8ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 36.7ms\n",
      "Speed: 3.6ms preprocess, 36.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 23.7ms\n",
      "Speed: 2.1ms preprocess, 23.7ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 41.5ms\n",
      "Speed: 3.4ms preprocess, 41.5ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 40.5ms\n",
      "Speed: 2.6ms preprocess, 40.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 10 cars, 25.9ms\n",
      "Speed: 2.6ms preprocess, 25.9ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 10 cars, 17.7ms\n",
      "Speed: 2.0ms preprocess, 17.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 10 cars, 22.2ms\n",
      "Speed: 2.0ms preprocess, 22.2ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 30.0ms\n",
      "Speed: 2.5ms preprocess, 30.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 30.8ms\n",
      "Speed: 2.2ms preprocess, 30.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 34.6ms\n",
      "Speed: 2.7ms preprocess, 34.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 36.7ms\n",
      "Speed: 3.4ms preprocess, 36.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 39.4ms\n",
      "Speed: 2.4ms preprocess, 39.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 33.6ms\n",
      "Speed: 3.4ms preprocess, 33.6ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 38.8ms\n",
      "Speed: 2.4ms preprocess, 38.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 32.8ms\n",
      "Speed: 3.3ms preprocess, 32.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 bus, 14.8ms\n",
      "Speed: 2.0ms preprocess, 14.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 bus, 16.4ms\n",
      "Speed: 2.5ms preprocess, 16.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 26.2ms\n",
      "Speed: 2.3ms preprocess, 26.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 17.9ms\n",
      "Speed: 2.4ms preprocess, 17.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 20.7ms\n",
      "Speed: 2.2ms preprocess, 20.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 17.3ms\n",
      "Speed: 2.2ms preprocess, 17.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 22.9ms\n",
      "Speed: 2.9ms preprocess, 22.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 22.3ms\n",
      "Speed: 2.8ms preprocess, 22.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 21.2ms\n",
      "Speed: 3.2ms preprocess, 21.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 14.4ms\n",
      "Speed: 2.1ms preprocess, 14.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 19.6ms\n",
      "Speed: 2.6ms preprocess, 19.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 29.4ms\n",
      "Speed: 3.1ms preprocess, 29.4ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 25.4ms\n",
      "Speed: 3.4ms preprocess, 25.4ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 20.8ms\n",
      "Speed: 2.1ms preprocess, 20.8ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 2 buss, 33.2ms\n",
      "Speed: 2.0ms preprocess, 33.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 42.2ms\n",
      "Speed: 3.3ms preprocess, 42.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 34.5ms\n",
      "Speed: 2.9ms preprocess, 34.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 37.4ms\n",
      "Speed: 2.4ms preprocess, 37.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 8.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 37.7ms\n",
      "Speed: 3.0ms preprocess, 37.7ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 31.5ms\n",
      "Speed: 3.5ms preprocess, 31.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 19.7ms\n",
      "Speed: 2.2ms preprocess, 19.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 17.3ms\n",
      "Speed: 2.0ms preprocess, 17.3ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 26.8ms\n",
      "Speed: 2.4ms preprocess, 26.8ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 40.6ms\n",
      "Speed: 3.4ms preprocess, 40.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 42.6ms\n",
      "Speed: 2.6ms preprocess, 42.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 35.9ms\n",
      "Speed: 3.2ms preprocess, 35.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 31.9ms\n",
      "Speed: 3.1ms preprocess, 31.9ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 16.1ms\n",
      "Speed: 2.0ms preprocess, 16.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 29.6ms\n",
      "Speed: 1.8ms preprocess, 29.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 44.3ms\n",
      "Speed: 3.9ms preprocess, 44.3ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 47.7ms\n",
      "Speed: 3.2ms preprocess, 47.7ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 26.2ms\n",
      "Speed: 2.1ms preprocess, 26.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 27.5ms\n",
      "Speed: 2.1ms preprocess, 27.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 14.4ms\n",
      "Speed: 2.0ms preprocess, 14.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 40.5ms\n",
      "Speed: 3.1ms preprocess, 40.5ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 23.4ms\n",
      "Speed: 2.1ms preprocess, 23.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 33.8ms\n",
      "Speed: 4.3ms preprocess, 33.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 34.0ms\n",
      "Speed: 3.3ms preprocess, 34.0ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 37.3ms\n",
      "Speed: 2.9ms preprocess, 37.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 31.5ms\n",
      "Speed: 3.5ms preprocess, 31.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 33.0ms\n",
      "Speed: 3.1ms preprocess, 33.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 26.1ms\n",
      "Speed: 2.2ms preprocess, 26.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 36.5ms\n",
      "Speed: 5.8ms preprocess, 36.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 22.2ms\n",
      "Speed: 1.9ms preprocess, 22.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 41.7ms\n",
      "Speed: 3.4ms preprocess, 41.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 43.0ms\n",
      "Speed: 3.3ms preprocess, 43.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 17.4ms\n",
      "Speed: 2.1ms preprocess, 17.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 34.8ms\n",
      "Speed: 3.1ms preprocess, 34.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 18.3ms\n",
      "Speed: 2.3ms preprocess, 18.3ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 38.5ms\n",
      "Speed: 3.5ms preprocess, 38.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 44.6ms\n",
      "Speed: 3.4ms preprocess, 44.6ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 39.7ms\n",
      "Speed: 3.9ms preprocess, 39.7ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 27.3ms\n",
      "Speed: 2.1ms preprocess, 27.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 13.9ms\n",
      "Speed: 2.0ms preprocess, 13.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 14.6ms\n",
      "Speed: 2.0ms preprocess, 14.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 40.3ms\n",
      "Speed: 2.2ms preprocess, 40.3ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 12.9ms\n",
      "Speed: 2.4ms preprocess, 12.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 31.3ms\n",
      "Speed: 2.9ms preprocess, 31.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 62.6ms\n",
      "Speed: 3.0ms preprocess, 62.6ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 47.1ms\n",
      "Speed: 4.3ms preprocess, 47.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 25.5ms\n",
      "Speed: 2.5ms preprocess, 25.5ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 28.6ms\n",
      "Speed: 3.7ms preprocess, 28.6ms inference, 10.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 37.8ms\n",
      "Speed: 3.4ms preprocess, 37.8ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 37.3ms\n",
      "Speed: 2.3ms preprocess, 37.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 27.6ms\n",
      "Speed: 3.2ms preprocess, 27.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 33.5ms\n",
      "Speed: 2.9ms preprocess, 33.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 train, 13.0ms\n",
      "Speed: 2.6ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 train, 20.2ms\n",
      "Speed: 2.3ms preprocess, 20.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 train, 19.2ms\n",
      "Speed: 3.2ms preprocess, 19.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 train, 24.3ms\n",
      "Speed: 2.4ms preprocess, 24.3ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 train, 33.4ms\n",
      "Speed: 2.9ms preprocess, 33.4ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 train, 36.9ms\n",
      "Speed: 2.9ms preprocess, 36.9ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 train, 29.9ms\n",
      "Speed: 2.1ms preprocess, 29.9ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 train, 29.4ms\n",
      "Speed: 3.0ms preprocess, 29.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 29.3ms\n",
      "Speed: 4.1ms preprocess, 29.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 train, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 train, 30.7ms\n",
      "Speed: 2.8ms preprocess, 30.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 train, 32.5ms\n",
      "Speed: 3.0ms preprocess, 32.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 train, 24.0ms\n",
      "Speed: 2.9ms preprocess, 24.0ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 train, 35.4ms\n",
      "Speed: 2.5ms preprocess, 35.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 37.5ms\n",
      "Speed: 2.4ms preprocess, 37.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 31.6ms\n",
      "Speed: 3.3ms preprocess, 31.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 14.8ms\n",
      "Speed: 2.1ms preprocess, 14.8ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 30.2ms\n",
      "Speed: 3.2ms preprocess, 30.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 23.1ms\n",
      "Speed: 2.9ms preprocess, 23.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 34.2ms\n",
      "Speed: 2.0ms preprocess, 34.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 30.0ms\n",
      "Speed: 2.7ms preprocess, 30.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 31.2ms\n",
      "Speed: 2.7ms preprocess, 31.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 38.5ms\n",
      "Speed: 3.0ms preprocess, 38.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 36.5ms\n",
      "Speed: 3.2ms preprocess, 36.5ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 1 truck, 24.2ms\n",
      "Speed: 2.7ms preprocess, 24.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 26.2ms\n",
      "Speed: 2.8ms preprocess, 26.2ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 31.4ms\n",
      "Speed: 3.1ms preprocess, 31.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 19.0ms\n",
      "Speed: 1.9ms preprocess, 19.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 50.5ms\n",
      "Speed: 2.7ms preprocess, 50.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 17.1ms\n",
      "Speed: 2.5ms preprocess, 17.1ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 23.7ms\n",
      "Speed: 3.7ms preprocess, 23.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 32.3ms\n",
      "Speed: 3.3ms preprocess, 32.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 29.1ms\n",
      "Speed: 2.1ms preprocess, 29.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 32.9ms\n",
      "Speed: 2.6ms preprocess, 32.9ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 22.8ms\n",
      "Speed: 3.0ms preprocess, 22.8ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 39.5ms\n",
      "Speed: 3.3ms preprocess, 39.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 1 truck, 28.6ms\n",
      "Speed: 3.1ms preprocess, 28.6ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 21.6ms\n",
      "Speed: 2.3ms preprocess, 21.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 14.9ms\n",
      "Speed: 2.7ms preprocess, 14.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 16.1ms\n",
      "Speed: 2.3ms preprocess, 16.1ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 20.1ms\n",
      "Speed: 3.3ms preprocess, 20.1ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 42.5ms\n",
      "Speed: 2.9ms preprocess, 42.5ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 37.6ms\n",
      "Speed: 4.9ms preprocess, 37.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 27.2ms\n",
      "Speed: 2.9ms preprocess, 27.2ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 13.5ms\n",
      "Speed: 2.1ms preprocess, 13.5ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 35.2ms\n",
      "Speed: 2.9ms preprocess, 35.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 16.7ms\n",
      "Speed: 2.3ms preprocess, 16.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 33.2ms\n",
      "Speed: 3.5ms preprocess, 33.2ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 17.2ms\n",
      "Speed: 2.3ms preprocess, 17.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 train, 39.6ms\n",
      "Speed: 2.9ms preprocess, 39.6ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 train, 28.1ms\n",
      "Speed: 2.9ms preprocess, 28.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 1 train, 27.8ms\n",
      "Speed: 2.3ms preprocess, 27.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 1 train, 24.4ms\n",
      "Speed: 2.4ms preprocess, 24.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 bus, 1 train, 31.2ms\n",
      "Speed: 3.5ms preprocess, 31.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 1 train, 29.2ms\n",
      "Speed: 2.5ms preprocess, 29.2ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 1 train, 17.2ms\n",
      "Speed: 2.3ms preprocess, 17.2ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 1 train, 34.6ms\n",
      "Speed: 3.2ms preprocess, 34.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 1 train, 22.0ms\n",
      "Speed: 2.2ms preprocess, 22.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 1 train, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 22.4ms\n",
      "Speed: 2.4ms preprocess, 22.4ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 35.9ms\n",
      "Speed: 3.2ms preprocess, 35.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 1 truck, 80.0ms\n",
      "Speed: 3.4ms preprocess, 80.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 12.7ms\n",
      "Speed: 2.4ms preprocess, 12.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 38.2ms\n",
      "Speed: 3.3ms preprocess, 38.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 21.5ms\n",
      "Speed: 2.1ms preprocess, 21.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 26.0ms\n",
      "Speed: 2.3ms preprocess, 26.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 27.5ms\n",
      "Speed: 2.9ms preprocess, 27.5ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 32.1ms\n",
      "Speed: 2.3ms preprocess, 32.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 41.2ms\n",
      "Speed: 3.0ms preprocess, 41.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 24.8ms\n",
      "Speed: 2.9ms preprocess, 24.8ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 30.2ms\n",
      "Speed: 2.8ms preprocess, 30.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 29.3ms\n",
      "Speed: 2.7ms preprocess, 29.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 31.8ms\n",
      "Speed: 3.2ms preprocess, 31.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 32.0ms\n",
      "Speed: 3.5ms preprocess, 32.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 11.8ms\n",
      "Speed: 2.1ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 cars, 1 train, 34.9ms\n",
      "Speed: 3.4ms preprocess, 34.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 cars, 1 train, 28.6ms\n",
      "Speed: 2.8ms preprocess, 28.6ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 cars, 1 train, 29.6ms\n",
      "Speed: 3.1ms preprocess, 29.6ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 44.1ms\n",
      "Speed: 3.8ms preprocess, 44.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 18.4ms\n",
      "Speed: 2.1ms preprocess, 18.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 train, 15.4ms\n",
      "Speed: 2.0ms preprocess, 15.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 17.4ms\n",
      "Speed: 1.9ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 train, 22.0ms\n",
      "Speed: 3.0ms preprocess, 22.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 36.4ms\n",
      "Speed: 2.6ms preprocess, 36.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 48.2ms\n",
      "Speed: 3.3ms preprocess, 48.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 39.8ms\n",
      "Speed: 2.5ms preprocess, 39.8ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 16.5ms\n",
      "Speed: 2.2ms preprocess, 16.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 30.3ms\n",
      "Speed: 3.0ms preprocess, 30.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 train, 22.1ms\n",
      "Speed: 2.4ms preprocess, 22.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 33.3ms\n",
      "Speed: 2.4ms preprocess, 33.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 30.0ms\n",
      "Speed: 2.3ms preprocess, 30.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 train, 32.1ms\n",
      "Speed: 3.1ms preprocess, 32.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Load YOLO and MiDaS ---\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# --- Load ROI ---\n",
    "try:\n",
    "    roi_polygon = np.load(\"roi_points.npy\").tolist()\n",
    "except:\n",
    "    print(\"ROI points file not found.\")\n",
    "    exit()\n",
    "\n",
    "roi_np = np.array(roi_polygon, dtype=np.int32)\n",
    "\n",
    "# --- Video Input ---\n",
    "cap = cv2.VideoCapture(\"traffic testvideo.mp4\")\n",
    "VEHICLE_CLASSES = [2, 3, 5, 7]  # car, motorcycle, bus, truck\n",
    "\n",
    "# --- Fuzzy Logic Setup ---\n",
    "vehicle_count_input = ctrl.Antecedent(np.arange(0, 50, 1), 'vehicle_count')\n",
    "queue_length_input = ctrl.Antecedent(np.arange(0, 100, 1), 'queue_length')\n",
    "green_time_output = ctrl.Consequent(np.arange(10, 70, 1), 'green_time')\n",
    "\n",
    "# Membership functions\n",
    "vehicle_count_input.automf(3)  # low, medium, high\n",
    "queue_length_input.automf(3)\n",
    "green_time_output['short'] = fuzz.trimf(green_time_output.universe, [10, 10, 30])\n",
    "green_time_output['medium'] = fuzz.trimf(green_time_output.universe, [20, 40, 60])\n",
    "green_time_output['long'] = fuzz.trimf(green_time_output.universe, [40, 60, 70])\n",
    "\n",
    "# Fuzzy Rules\n",
    "rules = [\n",
    "    ctrl.Rule(vehicle_count_input['poor'] & queue_length_input['poor'], green_time_output['short']),\n",
    "    ctrl.Rule(vehicle_count_input['average'] | queue_length_input['average'], green_time_output['medium']),\n",
    "    ctrl.Rule(vehicle_count_input['good'] | queue_length_input['good'], green_time_output['long']),\n",
    "]\n",
    "fuzzy_ctrl = ctrl.ControlSystem(rules)\n",
    "fuzzy_sim = ctrl.ControlSystemSimulation(fuzzy_ctrl)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    overlay_frame = frame.copy()\n",
    "    results = model.track(frame, persist=True, tracker=\"bytetrack.yaml\")[0]\n",
    "    current_ids = set()\n",
    "    vehicle_centers = []\n",
    "    center_depths = []\n",
    "\n",
    "    if results.boxes.id is not None:\n",
    "        ids = results.boxes.id.cpu().numpy()\n",
    "        boxes = results.boxes.xyxy.cpu().numpy()\n",
    "        class_ids = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "        for box, track_id, cls in zip(boxes, ids, class_ids):\n",
    "            if int(cls) not in VEHICLE_CLASSES:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "            if cv2.pointPolygonTest(roi_np, center, False) >= 0:\n",
    "                current_ids.add(track_id)\n",
    "                vehicle_centers.append(center)\n",
    "                cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(overlay_frame, f'ID: {int(track_id)}', (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "                cv2.circle(overlay_frame, center, 4, (255, 0, 0), -1)\n",
    "\n",
    "    vehicle_count = len(vehicle_centers)\n",
    "\n",
    "    # === MiDaS Depth Estimation ===\n",
    "    input_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = midas_transforms(input_rgb).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        depth = midas(input_tensor)\n",
    "        depth = torch.nn.functional.interpolate(\n",
    "            depth.unsqueeze(1), size=frame.shape[:2], mode=\"bicubic\", align_corners=False\n",
    "        ).squeeze()\n",
    "\n",
    "    depth_np = depth.cpu().numpy()\n",
    "    depth_display = cv2.normalize(depth_np, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # --- Depth Estimation from Polygon Points (3rd and 4th Points) ---\n",
    "    # Extract 3rd and 4th points (near the camera)\n",
    "    start_point = tuple(roi_np[2])  # 3rd point (index 2)\n",
    "    end_point = tuple(roi_np[3])    # 4th point (index 3)\n",
    "\n",
    "    # Draw a line between the 3rd and 4th points (for visualization)\n",
    "    cv2.line(frame, start_point, end_point, (255, 0, 0), 3)\n",
    "\n",
    "    # Masking the depth around the region of interest\n",
    "    mask = np.zeros_like(depth_display)\n",
    "    cv2.fillPoly(mask, [roi_np], 255)\n",
    "    masked_depth = cv2.bitwise_and(depth_display, mask)\n",
    "\n",
    "    for center in vehicle_centers:\n",
    "        x, y = center\n",
    "        if 0 <= y < depth_np.shape[0] and 0 <= x < depth_np.shape[1]:\n",
    "            center_depths.append(depth_np[y, x])\n",
    "\n",
    "    # === Calculate Queue Length Only if Vehicle is Near the Camera ===\n",
    "    if center_depths:\n",
    "        # Set a threshold depth for vehicles near the camera\n",
    "        depth_threshold = 100  # Arbitrary threshold for depth near camera\n",
    "        near_vehicle_depths = [depth for depth in center_depths if depth < depth_threshold]\n",
    "\n",
    "        if near_vehicle_depths:\n",
    "            nearest = min(near_vehicle_depths)\n",
    "            farthest = max(near_vehicle_depths)\n",
    "            queue_length = farthest - nearest\n",
    "        else:\n",
    "            queue_length = 0\n",
    "    else:\n",
    "        queue_length = 0\n",
    "\n",
    "    # === Fuzzy Inference ===\n",
    "    fuzzy_sim.input['vehicle_count'] = min(vehicle_count, 49)\n",
    "    fuzzy_sim.input['queue_length'] = min(queue_length, 99)\n",
    "    fuzzy_sim.compute()\n",
    "    green_signal_time = int(fuzzy_sim.output['green_time'])\n",
    "\n",
    "    # === Overlay Display ===\n",
    "    heatmap = cv2.applyColorMap(masked_depth, cv2.COLORMAP_JET)\n",
    "    combined = cv2.addWeighted(overlay_frame, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    cv2.polylines(combined, [roi_np], True, (255, 255, 255), 2)\n",
    "    cv2.putText(combined, f\"Vehicles: {vehicle_count}\", (30, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "    cv2.putText(combined, f\"Queue Length: {queue_length:.2f}\", (30, 80),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "    cv2.putText(combined, f\"Green Signal Time: {green_signal_time}s\", (30, 120),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 180, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"YOLOv8 + MiDaS + Fuzzy System\", combined)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed07d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "/home/santhosh/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/santhosh/.local/lib/python3.10/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 cars, 1 truck, 32.8ms\n",
      "Speed: 8.4ms preprocess, 32.8ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 26.3ms\n",
      "Speed: 3.7ms preprocess, 26.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 bus, 1 truck, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 bus, 1 truck, 8.6ms\n",
      "Speed: 1.4ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 11.9ms\n",
      "Speed: 1.7ms preprocess, 11.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 14.7ms\n",
      "Speed: 1.9ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 42.8ms\n",
      "Speed: 2.8ms preprocess, 42.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 truck, 58.8ms\n",
      "Speed: 2.7ms preprocess, 58.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 11.4ms\n",
      "Speed: 1.5ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 31.1ms\n",
      "Speed: 3.3ms preprocess, 31.1ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 21.0ms\n",
      "Speed: 1.5ms preprocess, 21.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 36.8ms\n",
      "Speed: 1.6ms preprocess, 36.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 40.4ms\n",
      "Speed: 3.7ms preprocess, 40.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 truck, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 28.7ms\n",
      "Speed: 2.4ms preprocess, 28.7ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 29.6ms\n",
      "Speed: 2.1ms preprocess, 29.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 23.6ms\n",
      "Speed: 2.3ms preprocess, 23.6ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 15.6ms\n",
      "Speed: 1.6ms preprocess, 15.6ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 10.5ms\n",
      "Speed: 1.6ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 18.4ms\n",
      "Speed: 1.7ms preprocess, 18.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 10.6ms\n",
      "Speed: 1.5ms preprocess, 10.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 9.0ms\n",
      "Speed: 1.2ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 18.4ms\n",
      "Speed: 2.0ms preprocess, 18.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 train, 11.6ms\n",
      "Speed: 2.2ms preprocess, 11.6ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 train, 44.4ms\n",
      "Speed: 2.6ms preprocess, 44.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 48.3ms\n",
      "Speed: 2.6ms preprocess, 48.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 30.3ms\n",
      "Speed: 2.0ms preprocess, 30.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 35.0ms\n",
      "Speed: 2.6ms preprocess, 35.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 15.4ms\n",
      "Speed: 1.9ms preprocess, 15.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 7.4ms\n",
      "Speed: 1.3ms preprocess, 7.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 1.6ms preprocess, 11.1ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 2.4ms preprocess, 37.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.7ms\n",
      "Speed: 2.5ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.8ms\n",
      "Speed: 3.7ms preprocess, 23.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 17.9ms\n",
      "Speed: 1.3ms preprocess, 17.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 51.9ms\n",
      "Speed: 4.0ms preprocess, 51.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 25.4ms\n",
      "Speed: 1.7ms preprocess, 25.4ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.3ms\n",
      "Speed: 1.3ms preprocess, 7.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 2.2ms preprocess, 35.9ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 2.4ms preprocess, 35.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 21.0ms\n",
      "Speed: 2.8ms preprocess, 21.0ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.5ms\n",
      "Speed: 1.3ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14.5ms\n",
      "Speed: 2.2ms preprocess, 14.5ms inference, 9.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.2ms\n",
      "Speed: 1.5ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 31.4ms\n",
      "Speed: 2.5ms preprocess, 31.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 57.1ms\n",
      "Speed: 2.9ms preprocess, 57.1ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 1.4ms preprocess, 8.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.5ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 31.7ms\n",
      "Speed: 2.4ms preprocess, 31.7ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.8ms\n",
      "Speed: 1.4ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 23.6ms\n",
      "Speed: 2.5ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 20.4ms\n",
      "Speed: 1.7ms preprocess, 20.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 20.0ms\n",
      "Speed: 2.6ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 31.8ms\n",
      "Speed: 1.8ms preprocess, 31.8ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.1ms\n",
      "Speed: 1.3ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 1.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 20.1ms\n",
      "Speed: 2.0ms preprocess, 20.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.2ms\n",
      "Speed: 1.3ms preprocess, 8.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 16.2ms\n",
      "Speed: 1.4ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.4ms\n",
      "Speed: 1.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 21.3ms\n",
      "Speed: 1.5ms preprocess, 21.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.2ms\n",
      "Speed: 2.4ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 22.7ms\n",
      "Speed: 2.0ms preprocess, 22.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 50.2ms\n",
      "Speed: 2.3ms preprocess, 50.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 2.2ms preprocess, 37.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 20.1ms\n",
      "Speed: 2.0ms preprocess, 20.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 2.6ms preprocess, 35.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 1.4ms preprocess, 11.1ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 1.4ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 24.8ms\n",
      "Speed: 1.6ms preprocess, 24.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 19.9ms\n",
      "Speed: 2.2ms preprocess, 19.9ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 19.3ms\n",
      "Speed: 1.5ms preprocess, 19.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.6ms\n",
      "Speed: 1.8ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10.8ms\n",
      "Speed: 1.5ms preprocess, 10.8ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 1.9ms preprocess, 12.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 28.1ms\n",
      "Speed: 2.9ms preprocess, 28.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 1.9ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 19.8ms\n",
      "Speed: 1.7ms preprocess, 19.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 2.1ms preprocess, 37.7ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.3ms\n",
      "Speed: 1.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 1.4ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.1ms preprocess, 13.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 2.6ms preprocess, 34.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 17.5ms\n",
      "Speed: 1.6ms preprocess, 17.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 1.6ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 15.5ms\n",
      "Speed: 1.3ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 48.6ms\n",
      "Speed: 3.1ms preprocess, 48.6ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 39.3ms\n",
      "Speed: 1.5ms preprocess, 39.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 11.5ms\n",
      "Speed: 1.6ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 8.8ms\n",
      "Speed: 2.6ms preprocess, 8.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 23.6ms\n",
      "Speed: 1.4ms preprocess, 23.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 7.9ms\n",
      "Speed: 1.5ms preprocess, 7.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 17.8ms\n",
      "Speed: 1.4ms preprocess, 17.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 12.6ms\n",
      "Speed: 1.6ms preprocess, 12.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 14.0ms\n",
      "Speed: 1.6ms preprocess, 14.0ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 11.1ms\n",
      "Speed: 1.6ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 12.1ms\n",
      "Speed: 1.6ms preprocess, 12.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 1 truck, 22.5ms\n",
      "Speed: 1.7ms preprocess, 22.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 cars, 1 truck, 11.8ms\n",
      "Speed: 1.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 cars, 1 truck, 10.7ms\n",
      "Speed: 1.8ms preprocess, 10.7ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 cars, 1 truck, 19.0ms\n",
      "Speed: 2.3ms preprocess, 19.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 3 cars, 40.5ms\n",
      "Speed: 1.6ms preprocess, 40.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 cars, 28.8ms\n",
      "Speed: 3.9ms preprocess, 28.8ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 4 cars, 41.9ms\n",
      "Speed: 3.5ms preprocess, 41.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 4 cars, 27.8ms\n",
      "Speed: 2.3ms preprocess, 27.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 4 cars, 27.6ms\n",
      "Speed: 1.7ms preprocess, 27.6ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 4 cars, 13.2ms\n",
      "Speed: 1.7ms preprocess, 13.2ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 4 cars, 28.7ms\n",
      "Speed: 1.5ms preprocess, 28.7ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 5 cars, 16.7ms\n",
      "Speed: 2.0ms preprocess, 16.7ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 5 cars, 15.4ms\n",
      "Speed: 1.9ms preprocess, 15.4ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 5 cars, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 32.4ms\n",
      "Speed: 1.6ms preprocess, 32.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 16.1ms\n",
      "Speed: 1.9ms preprocess, 16.1ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 10.4ms\n",
      "Speed: 1.7ms preprocess, 10.4ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 27.8ms\n",
      "Speed: 1.6ms preprocess, 27.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 1 truck, 26.4ms\n",
      "Speed: 1.5ms preprocess, 26.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 1 truck, 11.3ms\n",
      "Speed: 1.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 7.4ms\n",
      "Speed: 1.4ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 22.3ms\n",
      "Speed: 1.5ms preprocess, 22.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 36.9ms\n",
      "Speed: 2.5ms preprocess, 36.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 39.0ms\n",
      "Speed: 2.4ms preprocess, 39.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 21.4ms\n",
      "Speed: 2.4ms preprocess, 21.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 11.2ms\n",
      "Speed: 1.5ms preprocess, 11.2ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 22.0ms\n",
      "Speed: 1.8ms preprocess, 22.0ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 29.2ms\n",
      "Speed: 2.7ms preprocess, 29.2ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 11.0ms\n",
      "Speed: 1.4ms preprocess, 11.0ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 42.5ms\n",
      "Speed: 1.8ms preprocess, 42.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 9.9ms\n",
      "Speed: 1.9ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 36.2ms\n",
      "Speed: 2.5ms preprocess, 36.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 13.6ms\n",
      "Speed: 1.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 10.2ms\n",
      "Speed: 1.7ms preprocess, 10.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 45.0ms\n",
      "Speed: 3.2ms preprocess, 45.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 14.6ms\n",
      "Speed: 2.2ms preprocess, 14.6ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 36.5ms\n",
      "Speed: 2.7ms preprocess, 36.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 11.3ms\n",
      "Speed: 1.5ms preprocess, 11.3ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 38.5ms\n",
      "Speed: 2.1ms preprocess, 38.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 21.9ms\n",
      "Speed: 2.3ms preprocess, 21.9ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 7.9ms\n",
      "Speed: 1.5ms preprocess, 7.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 49.9ms\n",
      "Speed: 2.8ms preprocess, 49.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 22.9ms\n",
      "Speed: 2.7ms preprocess, 22.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 11.8ms\n",
      "Speed: 1.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 60.8ms\n",
      "Speed: 1.6ms preprocess, 60.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 37.4ms\n",
      "Speed: 1.7ms preprocess, 37.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 21.4ms\n",
      "Speed: 2.0ms preprocess, 21.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 bus, 12.7ms\n",
      "Speed: 1.8ms preprocess, 12.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 bus, 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 truck, 15.3ms\n",
      "Speed: 1.4ms preprocess, 15.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 1 truck, 23.6ms\n",
      "Speed: 1.6ms preprocess, 23.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 27.0ms\n",
      "Speed: 1.9ms preprocess, 27.0ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 12.0ms\n",
      "Speed: 1.7ms preprocess, 12.0ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 31.3ms\n",
      "Speed: 2.5ms preprocess, 31.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 12.9ms\n",
      "Speed: 1.4ms preprocess, 12.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 34.8ms\n",
      "Speed: 1.5ms preprocess, 34.8ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6 cars, 1 truck, 7.7ms\n",
      "Speed: 1.2ms preprocess, 7.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 11.9ms\n",
      "Speed: 1.8ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 10.5ms\n",
      "Speed: 1.2ms preprocess, 10.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 7.5ms\n",
      "Speed: 1.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 18.1ms\n",
      "Speed: 1.8ms preprocess, 18.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 7.9ms\n",
      "Speed: 1.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 28.7ms\n",
      "Speed: 1.3ms preprocess, 28.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 20.9ms\n",
      "Speed: 1.4ms preprocess, 20.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 10.9ms\n",
      "Speed: 1.7ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 22.0ms\n",
      "Speed: 1.9ms preprocess, 22.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 28.7ms\n",
      "Speed: 2.1ms preprocess, 28.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 44.1ms\n",
      "Speed: 1.9ms preprocess, 44.1ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 27.3ms\n",
      "Speed: 1.2ms preprocess, 27.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 15.8ms\n",
      "Speed: 1.6ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 8.3ms\n",
      "Speed: 1.7ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 22.8ms\n",
      "Speed: 2.5ms preprocess, 22.8ms inference, 8.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 15.0ms\n",
      "Speed: 1.3ms preprocess, 15.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 11.8ms\n",
      "Speed: 1.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 15.5ms\n",
      "Speed: 1.5ms preprocess, 15.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 27.9ms\n",
      "Speed: 1.7ms preprocess, 27.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 28.7ms\n",
      "Speed: 2.2ms preprocess, 28.7ms inference, 11.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 28.6ms\n",
      "Speed: 3.5ms preprocess, 28.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 13.7ms\n",
      "Speed: 1.5ms preprocess, 13.7ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 23.2ms\n",
      "Speed: 3.7ms preprocess, 23.2ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 2 trucks, 26.3ms\n",
      "Speed: 3.5ms preprocess, 26.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 1 truck, 12.9ms\n",
      "Speed: 1.4ms preprocess, 12.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 8 cars, 1 truck, 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 1 truck, 10.7ms\n",
      "Speed: 1.8ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 8.0ms\n",
      "Speed: 1.4ms preprocess, 8.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 1 truck, 24.9ms\n",
      "Speed: 2.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 truck, 25.6ms\n",
      "Speed: 1.9ms preprocess, 25.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 truck, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 24.5ms\n",
      "Speed: 1.3ms preprocess, 24.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 24.0ms\n",
      "Speed: 2.7ms preprocess, 24.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 7.2ms\n",
      "Speed: 1.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 21.4ms\n",
      "Speed: 2.2ms preprocess, 21.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 18.6ms\n",
      "Speed: 1.7ms preprocess, 18.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 39.1ms\n",
      "Speed: 2.0ms preprocess, 39.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 9.6ms\n",
      "Speed: 1.3ms preprocess, 9.6ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 13.9ms\n",
      "Speed: 1.2ms preprocess, 13.9ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 48.8ms\n",
      "Speed: 1.9ms preprocess, 48.8ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 16.5ms\n",
      "Speed: 1.5ms preprocess, 16.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 26.7ms\n",
      "Speed: 2.9ms preprocess, 26.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 10.4ms\n",
      "Speed: 2.0ms preprocess, 10.4ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 8.7ms\n",
      "Speed: 1.8ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 13.4ms\n",
      "Speed: 1.5ms preprocess, 13.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 10.8ms\n",
      "Speed: 1.5ms preprocess, 10.8ms inference, 13.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 31.3ms\n",
      "Speed: 3.0ms preprocess, 31.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 23.6ms\n",
      "Speed: 2.2ms preprocess, 23.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 bus, 1 truck, 11.9ms\n",
      "Speed: 1.8ms preprocess, 11.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 17.2ms\n",
      "Speed: 1.5ms preprocess, 17.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 34.0ms\n",
      "Speed: 1.6ms preprocess, 34.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 10.3ms\n",
      "Speed: 1.6ms preprocess, 10.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 8.9ms\n",
      "Speed: 1.3ms preprocess, 8.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 8.7ms\n",
      "Speed: 1.3ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 10.1ms\n",
      "Speed: 1.3ms preprocess, 10.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 8.1ms\n",
      "Speed: 1.5ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 35.1ms\n",
      "Speed: 3.0ms preprocess, 35.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 25.8ms\n",
      "Speed: 2.3ms preprocess, 25.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 10.0ms\n",
      "Speed: 1.7ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 11.0ms\n",
      "Speed: 1.5ms preprocess, 11.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 39.2ms\n",
      "Speed: 1.6ms preprocess, 39.2ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 28.6ms\n",
      "Speed: 1.5ms preprocess, 28.6ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 32.9ms\n",
      "Speed: 2.8ms preprocess, 32.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 19.5ms\n",
      "Speed: 2.0ms preprocess, 19.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 51.8ms\n",
      "Speed: 1.7ms preprocess, 51.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 20.2ms\n",
      "Speed: 1.8ms preprocess, 20.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 13.5ms\n",
      "Speed: 1.9ms preprocess, 13.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 43.6ms\n",
      "Speed: 3.3ms preprocess, 43.6ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 16.6ms\n",
      "Speed: 1.5ms preprocess, 16.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 11.1ms\n",
      "Speed: 1.9ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 28.6ms\n",
      "Speed: 2.4ms preprocess, 28.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 11.8ms\n",
      "Speed: 1.4ms preprocess, 11.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 12.7ms\n",
      "Speed: 1.5ms preprocess, 12.7ms inference, 13.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 7.4ms\n",
      "Speed: 1.3ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 11.4ms\n",
      "Speed: 1.8ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 14.1ms\n",
      "Speed: 1.6ms preprocess, 14.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 27.1ms\n",
      "Speed: 1.6ms preprocess, 27.1ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 20.0ms\n",
      "Speed: 4.5ms preprocess, 20.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 16.3ms\n",
      "Speed: 1.6ms preprocess, 16.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 11.1ms\n",
      "Speed: 1.3ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 14.8ms\n",
      "Speed: 1.3ms preprocess, 14.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 29.6ms\n",
      "Speed: 3.7ms preprocess, 29.6ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 24.1ms\n",
      "Speed: 2.3ms preprocess, 24.1ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 8.5ms\n",
      "Speed: 1.5ms preprocess, 8.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 37.1ms\n",
      "Speed: 1.8ms preprocess, 37.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 14.9ms\n",
      "Speed: 1.6ms preprocess, 14.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 7.4ms\n",
      "Speed: 1.4ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 20.2ms\n",
      "Speed: 1.7ms preprocess, 20.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 18.6ms\n",
      "Speed: 2.0ms preprocess, 18.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 19.2ms\n",
      "Speed: 2.1ms preprocess, 19.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 bus, 45.9ms\n",
      "Speed: 2.3ms preprocess, 45.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 25.3ms\n",
      "Speed: 1.2ms preprocess, 25.3ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 8.5ms\n",
      "Speed: 1.7ms preprocess, 8.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 10.6ms\n",
      "Speed: 1.7ms preprocess, 10.6ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 14.1ms\n",
      "Speed: 2.0ms preprocess, 14.1ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 38.8ms\n",
      "Speed: 1.6ms preprocess, 38.8ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 10.8ms\n",
      "Speed: 1.3ms preprocess, 10.8ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 13.4ms\n",
      "Speed: 2.0ms preprocess, 13.4ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 26.0ms\n",
      "Speed: 2.6ms preprocess, 26.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 10.6ms\n",
      "Speed: 1.6ms preprocess, 10.6ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 43.2ms\n",
      "Speed: 3.9ms preprocess, 43.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 10.9ms\n",
      "Speed: 1.8ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 8.2ms\n",
      "Speed: 1.8ms preprocess, 8.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 16.2ms\n",
      "Speed: 1.4ms preprocess, 16.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 11.1ms\n",
      "Speed: 3.7ms preprocess, 11.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 7.0ms\n",
      "Speed: 1.3ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 8.5ms\n",
      "Speed: 1.6ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 15.4ms\n",
      "Speed: 1.4ms preprocess, 15.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 36.4ms\n",
      "Speed: 5.0ms preprocess, 36.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m\n\u001b[1;32m     92\u001b[0m     depth \u001b[38;5;241m=\u001b[39m midas(input_tensor)\n\u001b[1;32m     93\u001b[0m     depth \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m     94\u001b[0m         depth\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), size\u001b[38;5;241m=\u001b[39mframe\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     )\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 97\u001b[0m depth_np \u001b[38;5;241m=\u001b[39m \u001b[43mdepth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     98\u001b[0m depth_display \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mnormalize(depth_np, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mNORM_MINMAX)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Mask for ROI\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Load Models ---\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# --- Load ROI ---\n",
    "try:\n",
    "    roi_polygon = np.load(\"roi_points.npy\").tolist()\n",
    "except:\n",
    "    print(\"ROI points file not found.\")\n",
    "    exit()\n",
    "\n",
    "roi_np = np.array(roi_polygon, dtype=np.int32)\n",
    "\n",
    "# --- Video ---\n",
    "cap = cv2.VideoCapture(\"traffic testvideo.mp4\")\n",
    "VEHICLE_CLASSES = [2, 3, 5, 7]  # car, motorcycle, bus, truck\n",
    "\n",
    "# Weighting factors\n",
    "count_weight = 2\n",
    "depth_weight = 0.05\n",
    "\n",
    "# Green signal duration bounds\n",
    "min_green = 10  # seconds\n",
    "max_green = 60  # seconds\n",
    "\n",
    "def enhance_frame(frame):\n",
    "    # CLAHE (Contrast enhancement)\n",
    "    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    frame_clahe = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Sharpening\n",
    "    sharpen_kernel = np.array([[0, -1, 0],\n",
    "                               [-1, 5,-1],\n",
    "                               [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(frame_clahe, -1, sharpen_kernel)\n",
    "    return sharpened\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    overlay_frame = frame.copy()\n",
    "\n",
    "    # === YOLOv8 VEHICLE DETECTION ===\n",
    "    results = model.track(frame, persist=True, tracker=\"bytetrack.yaml\")[0]\n",
    "    current_ids = set()\n",
    "    vehicle_centers = []\n",
    "    center_depths = []\n",
    "\n",
    "    if results.boxes.id is not None:\n",
    "        ids = results.boxes.id.cpu().numpy()\n",
    "        boxes = results.boxes.xyxy.cpu().numpy()\n",
    "        class_ids = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "        for box, track_id, cls in zip(boxes, ids, class_ids):\n",
    "            if int(cls) not in VEHICLE_CLASSES:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "            if cv2.pointPolygonTest(roi_np, center, False) >= 0:\n",
    "                current_ids.add(track_id)\n",
    "                vehicle_centers.append(center)\n",
    "                cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(overlay_frame, f'ID: {int(track_id)}', (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "                cv2.circle(overlay_frame, center, 4, (255, 0, 0), -1)\n",
    "\n",
    "    vehicle_count = len(vehicle_centers)\n",
    "\n",
    "    # === MiDaS DEPTH ESTIMATION ===\n",
    "    enhanced_frame = enhance_frame(frame)\n",
    "    input_rgb = cv2.cvtColor(enhanced_frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = midas_transforms(input_rgb).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        depth = midas(input_tensor)\n",
    "        depth = torch.nn.functional.interpolate(\n",
    "            depth.unsqueeze(1), size=frame.shape[:2], mode=\"bicubic\", align_corners=False\n",
    "        ).squeeze()\n",
    "\n",
    "    depth_np = depth.cpu().numpy()\n",
    "    depth_display = cv2.normalize(depth_np, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Mask for ROI\n",
    "    mask = np.zeros_like(depth_display)\n",
    "    cv2.fillPoly(mask, [roi_np], 255)\n",
    "    masked_depth = cv2.bitwise_and(depth_display, mask)\n",
    "\n",
    "    # Collect depth values at vehicle centers\n",
    "    for center in vehicle_centers:\n",
    "        x, y = center\n",
    "        if 0 <= y < depth_np.shape[0] and 0 <= x < depth_np.shape[1]:\n",
    "            center_depths.append(depth_np[y, x])\n",
    "\n",
    "    # === Queue Length Calculation ===\n",
    "    if center_depths:\n",
    "        filtered_depths = np.clip(center_depths, 0, np.percentile(center_depths, 90))\n",
    "        nearest_depth = np.min(filtered_depths)\n",
    "        farthest_depth = np.max(filtered_depths)\n",
    "        queue_length = farthest_depth - nearest_depth\n",
    "    else:\n",
    "        queue_length = 0\n",
    "        nearest_depth = 0\n",
    "        farthest_depth = 0\n",
    "\n",
    "    # === Density Score and Green Signal Time ===\n",
    "    density_score = (vehicle_count * count_weight) + (queue_length * depth_weight)\n",
    "    norm_score = min(density_score / 100, 1.0)\n",
    "    green_time = int(min_green + (max_green - min_green) * norm_score)\n",
    "\n",
    "    # === Overlay Heatmap ===\n",
    "    heatmap = cv2.applyColorMap(masked_depth, cv2.COLORMAP_JET)\n",
    "    combined = cv2.addWeighted(overlay_frame, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    # === Display Stats ===\n",
    "    cv2.polylines(combined, [roi_np], True, (255, 255, 255), 2)\n",
    "    cv2.putText(combined, f\"Vehicles in ROI: {vehicle_count}\", (30, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "    cv2.putText(combined, f\"Queue Length: {queue_length:.2f}\", (30, 80),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "    cv2.putText(combined, f\"Density Score: {density_score:.2f}\", (30, 120),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 180, 255), 2)\n",
    "    cv2.putText(combined, f\"Nearest Depth: {nearest_depth:.2f}\", (30, 160),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (200, 255, 200), 2)\n",
    "    cv2.putText(combined, f\"Farthest Depth: {farthest_depth:.2f}\", (30, 200),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 200, 200), 2)\n",
    "    cv2.putText(combined, f\"Green Time: {green_time} sec\", (30, 240),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 180), 2)\n",
    "\n",
    "    cv2.imshow(\"YOLOv8 + MiDaS Heatmap Overlay\", combined)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77aa6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/facebookresearch_WSL-Images_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'source' is missing. Using 'source=/home/santhosh/.local/lib/python3.10/site-packages/ultralytics/assets'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "[ WARN:0@1147.250] global loadsave.cpp:268 findDecoder imread_('input_image.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/2 /home/santhosh/.local/lib/python3.10/site-packages/ultralytics/assets/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 89.7ms\n",
      "image 2/2 /home/santhosh/.local/lib/python3.10/site-packages/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 35.4ms\n",
      "Speed: 5.7ms preprocess, 62.6ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'midas.transforms' has no attribute 'Compose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m labels \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnames  \u001b[38;5;66;03m# Object labels\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 2. Apply MiDaS depth estimation\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m depth_transform \u001b[38;5;241m=\u001b[39m \u001b[43mmidas_transforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m([\n\u001b[1;32m     28\u001b[0m     midas_transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m384\u001b[39m),\n\u001b[1;32m     29\u001b[0m     midas_transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     30\u001b[0m     midas_transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[1;32m     31\u001b[0m ])\n\u001b[1;32m     32\u001b[0m input_image \u001b[38;5;241m=\u001b[39m depth_transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'midas.transforms' has no attribute 'Compose'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_model =  YOLO(\"yolov8n.pt\")\n",
    "# Load MiDaS model for depth estimation\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "# Load image (this will be your input frame)\n",
    "image = cv2.imread('input_image.jpg')\n",
    "\n",
    "# 1. Perform YOLOv8 detection\n",
    "results = yolo_model(image)\n",
    "\n",
    "# Access the results\n",
    "boxes = results[0].boxes  # This should give you the bounding boxes\n",
    "\n",
    "# For each detected box, the output should be [x1, y1, x2, y2, confidence, class_id]\n",
    "boxes_data = boxes.xyxy  # Bounding box coordinates in xyxy format\n",
    "\n",
    "# You can also access labels and confidence if needed\n",
    "labels = results[0].names  # Object labels\n",
    "\n",
    "# 2. Apply MiDaS depth estimation\n",
    "depth_transform = midas_transforms.Compose([\n",
    "    midas_transforms.Resize(384),\n",
    "    midas_transforms.ToTensor(),\n",
    "    midas_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_image = depth_transform(image).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    depth_map = midas(input_image)\n",
    "\n",
    "# Convert depth map to numpy for easier handling\n",
    "depth_map = depth_map.squeeze().cpu().numpy()\n",
    "\n",
    "# 3. For each detected vehicle (bounding box), check the depth\n",
    "for box in boxes_data:\n",
    "    x1, y1, x2, y2 = map(int, box[:4])  # Extract bounding box coordinates (x1, y1, x2, y2)\n",
    "    # Extract the depth of the area within the bounding box\n",
    "    depth_values = depth_map[y1:y2, x1:x2]\n",
    "\n",
    "    # Calculate the average depth in the bounding box region\n",
    "    avg_depth = np.mean(depth_values)\n",
    "\n",
    "    # Determine if the vehicle is closer or farther based on depth\n",
    "    if avg_depth < threshold:  # Assume threshold is set based on your depth range\n",
    "        print(f\"Vehicle at ({x1}, {y1}) is near.\")\n",
    "    else:\n",
    "        print(f\"Vehicle at ({x1}, {y1}) is far.\")\n",
    "\n",
    "# Optionally visualize the bounding boxes and depth info\n",
    "for box in boxes_data:\n",
    "    x1, y1, x2, y2 = map(int, box[:4])\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw box around detected vehicle\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow('Output', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e90f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/santhosh/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n",
      "Using cache found in /home/santhosh/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 bus, 2 trucks, 84.5ms\n",
      "Speed: 9.6ms preprocess, 84.5ms inference, 44.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 3, 129, 257]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m input_batch \u001b[38;5;241m=\u001b[39m midas_transform(img_rgb)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 40\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmidas\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m     42\u001b[0m         prediction\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     43\u001b[0m         size\u001b[38;5;241m=\u001b[39mframe\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     44\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m         align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m     )\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     47\u001b[0m depth_map \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/intel-isl_MiDaS_master/midas/midas_net_custom.py:87\u001b[0m, in \u001b[0;36mMidasNet_small.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.channels_last = \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels_last)\n\u001b[1;32m     84\u001b[0m     x\u001b[38;5;241m.\u001b[39mcontiguous(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mchannels_last)\n\u001b[0;32m---> 87\u001b[0m layer_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m layer_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mlayer2(layer_1)\n\u001b[1;32m     89\u001b[0m layer_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mlayer3(layer_2)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master/geffnet/conv2d_layers.py:112\u001b[0m, in \u001b[0;36mConv2dSameExport.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(x)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 3, 129, 257]"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Load MiDaS and transform\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas_transform = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").small_transform\n",
    "midas.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "\n",
    "# Define vehicle classes (COCO IDs)\n",
    "vehicle_classes = [2, 3, 5, 7]  # car, motorcycle, bus, truck\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(\"traffic testvideo.mp4\")  # Replace with 0 for webcam\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize for faster processing (optional)\n",
    "    original_frame = frame.copy()\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Step 1: YOLOv8 Detection\n",
    "    results = yolo_model(img_rgb)[0]\n",
    "    boxes = results.boxes.data.cpu().numpy()\n",
    "\n",
    "    vehicle_boxes = [box for box in boxes if int(box[5]) in vehicle_classes]\n",
    "\n",
    "    # Step 2: MiDaS Depth Estimation\n",
    "    input_batch = midas_transform(img_rgb).to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=frame.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "    depth_map = prediction.cpu().numpy()\n",
    "\n",
    "    # Step 3: Calculate depth of each vehicle (bottom center point)\n",
    "    queue_depths = []\n",
    "    for box in vehicle_boxes:\n",
    "        x1, y1, x2, y2 = map(int, box[:4])\n",
    "        cx = int((x1 + x2) / 2)\n",
    "        cy = y2  # bottom-center\n",
    "        if 0 <= cx < depth_map.shape[1] and 0 <= cy < depth_map.shape[0]:\n",
    "            depth = depth_map[cy, cx]\n",
    "            queue_depths.append(depth)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(original_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(original_frame, f\"{depth:.2f}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "    # Step 4: Estimate queue length\n",
    "    if queue_depths:\n",
    "        max_depth = max(queue_depths)\n",
    "        min_depth = min(queue_depths)\n",
    "        queue_length = max_depth - min_depth  # Relative queue depth\n",
    "\n",
    "        # Display queue length\n",
    "        cv2.putText(original_frame, f\"Queue Length (relative): {queue_length:.2f}\", (20, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    else:\n",
    "        cv2.putText(original_frame, \"No vehicles detected\", (20, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Show result\n",
    "    cv2.imshow(\"Queue Length Estimation\", original_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687b8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
